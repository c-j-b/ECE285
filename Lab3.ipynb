{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Tensors\n",
    "### 1. Construct a 5x3 tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 0.0000e+00  0.0000e+00  3.7036e-02\n",
      " 4.5694e-41  1.5636e-23  4.5694e-41\n",
      "-2.7390e+06  3.0826e-41 -2.7390e+06\n",
      " 3.0826e-41 -2.7390e+06  3.0826e-41\n",
      "-2.7391e+06  3.0826e-41  3.8388e-02\n",
      "[torch.FloatTensor of size 5x3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x = torch.Tensor(5,3)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How was x initialized? <br> \n",
    "> Values we're chosen from a uniform distribution between -1 and 1. The size is 5x3.<br>\n",
    "\n",
    "What is the type of x? <br>\n",
    "> torch.Tensor is an alias for torch.FloatTensor <br>\n",
    "\n",
    "### 2. Constuct a random matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 0.0867  0.4379  0.9489\n",
      " 0.7459  0.4695  0.3982\n",
      " 0.3757  0.7028  0.1211\n",
      " 0.0564  0.2084  0.1630\n",
      " 0.1114  0.0490  0.6380\n",
      "[torch.FloatTensor of size 5x3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y = torch.rand(5 ,3)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How are the values distributed? <br>\n",
    "> Randomly form a discrete uniform distribution with range [0, 2^mantissa]\n",
    "\n",
    "What type is y? <br>\n",
    "> Y is a torch.FloatTensor with size 5x3.\n",
    "\n",
    "What if we use torch.randn(5, 3) instead? <br>\n",
    "> The values of y are random numbers from a normal distribution with mean=0 and variance=1.\n",
    "\n",
    "## 3. Run the following"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x.double()\n",
    "y = y.double()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the type of x and y? <br>\n",
    "> x & y are type torch.DoubleTensor with size 5x3\n",
    "\n",
    "### 4. Init tensors with values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.Tensor([[-0.1859, 1.3970, 0.5236],\n",
    "                  [ 2.3854 , 0.0707 , 2.1970] ,\n",
    "                  [ -0.3587 , 1.2359 , 1.8951] ,\n",
    "                  [ -0.1189 , -0.1376, 0.4647],\n",
    "                  [ -1.8968 , 2.0164, 0.1092]])\n",
    "\n",
    "y = torch.Tensor([[0.4838, 0.5822, 0.2755],\n",
    "                  [1.0982, 0.4932, -0.6680],\n",
    "                  [0.7915, 0.6580, -0.5819],\n",
    "                  [0.3825, -1.1822, 1.5217],\n",
    "                  [0.6042, -0.2280, 1.3210]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is shape are x and y? <br>\n",
    "> Both x and y are shape 5x3.\n",
    "\n",
    "### 5. Stacking 2D tensors into 3D tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 5, 3])\n"
     ]
    }
   ],
   "source": [
    "z = torch.stack((x, y))\n",
    "print(z.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the shape of z? <br>\n",
    "> z has shape 2x5x3\n",
    "\n",
    "How does it compare to torch.cat((x,y), 0) and torch.cat((x,y), 1)?<br>\n",
    "> Torch.cat((x,y), 0) has size 10x3\n",
    "> Torch.cat((x,y), 1) has size 5x6\n",
    "\n",
    "### 6. Report the values\n",
    "Report the value of 5th row 3rd column from tensor y. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.32099997997\n"
     ]
    }
   ],
   "source": [
    "print(y[4, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Report the same value in z."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.32099997997\n"
     ]
    }
   ],
   "source": [
    "print(z[1, 4, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Print elements\n",
    "Print all elements corresponding to the 5th row and 3rd column in z. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 0.1092\n",
      " 1.3210\n",
      "[torch.FloatTensor of size 2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(z[:, 4, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many elements are there? <br>\n",
    "> 2 elements \n",
    "\n",
    "### 8. Adding tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 0.2979  1.9792  0.7991\n",
      " 3.4836  0.5639  1.5290\n",
      " 0.4328  1.8939  1.3132\n",
      " 0.2636 -1.3198  1.9864\n",
      "-1.2926  1.7884  1.4302\n",
      "[torch.FloatTensor of size 5x3]\n",
      "\n",
      "\n",
      " 0.2979  1.9792  0.7991\n",
      " 3.4836  0.5639  1.5290\n",
      " 0.4328  1.8939  1.3132\n",
      " 0.2636 -1.3198  1.9864\n",
      "-1.2926  1.7884  1.4302\n",
      "[torch.FloatTensor of size 5x3]\n",
      "\n",
      "\n",
      " 0.2979  1.9792  0.7991\n",
      " 3.4836  0.5639  1.5290\n",
      " 0.4328  1.8939  1.3132\n",
      " 0.2636 -1.3198  1.9864\n",
      "-1.2926  1.7884  1.4302\n",
      "[torch.FloatTensor of size 5x3]\n",
      "\n",
      "\n",
      " 0.2979  1.9792  0.7991\n",
      " 3.4836  0.5639  1.5290\n",
      " 0.4328  1.8939  1.3132\n",
      " 0.2636 -1.3198  1.9864\n",
      "-1.2926  1.7884  1.4302\n",
      "[torch.FloatTensor of size 5x3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(x + y)\n",
    "print(torch.add(x, y))\n",
    "print(x.add(y))\n",
    "torch.add(x, y, out = x)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check whether the above instructions are printing the same output? Are they equivalent? <br>\n",
    "> All methods of addition produce the same output.\n",
    "\n",
    "### 9. Reshaping tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 4]) torch.Size([16]) torch.Size([2, 8])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(4, 4)\n",
    "y = x.view(16)\n",
    "z = x.view(-1, 8)\n",
    "print(x.size(), y.size(), z.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpret the effect of each of these instructions. <br>\n",
    "> torch.randn(4,4): Initialize 4x4 tensor with random values. See above for randn. <br>\n",
    "> x.view(16): Reshape x into a 1x16 vector<br>\n",
    "> x.view(-1, 8): Reshape x into a vector with 8 as the second dimension. Let pytorch find the first dimension.<br>\n",
    "\n",
    "What does the -1 mean?\n",
    "> Let pytorch determine the required dimension to reshape the array using all elements. \n",
    "\n",
    "### 10. Resize tensors for matrix multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(10, 10)\n",
    "y = torch.randn(2, 100)\n",
    "z = torch.mm(x.view(1, 100), y.view(100, 2))\n",
    "print(z.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "N.B. torch.mm(NxM, MxP) -> NxP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 NumPy and PyTorch\n",
    "### 11. Converting torch tensor to numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 1\n",
      " 1\n",
      " 1\n",
      " 1\n",
      " 1\n",
      "[torch.FloatTensor of size 5]\n",
      "\n",
      "[1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones(5)\n",
    "print(a)\n",
    "b = a.numpy()\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comment<br>\n",
    "> The above code converts a torch.FloatTensor of 5 into a numpy ndarray with length 5.\n",
    "\n",
    "What are the types of a and b? <br>\n",
    "> a is a torch.FloatTensor <br>\n",
    "> b is a numpy.ndarray <br>\n",
    "\n",
    "### 12. Modifying tensors/np.arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 2\n",
      " 1\n",
      " 1\n",
      " 1\n",
      " 1\n",
      "[torch.FloatTensor of size 5]\n",
      "\n",
      "[2. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "a[0] += 1\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do they match? <br>\n",
    "> YES <br>\n",
    "\n",
    "Do they share their underlying memory locations? <br>\n",
    "> YES, one opperation can modify both values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13. Methods of adding and effects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 3\n",
      " 2\n",
      " 2\n",
      " 2\n",
      " 2\n",
      "[torch.FloatTensor of size 5]\n",
      "\n",
      "[3. 2. 2. 2. 2.]\n",
      "\n",
      " 4\n",
      " 3\n",
      " 3\n",
      " 3\n",
      " 3\n",
      "[torch.FloatTensor of size 5]\n",
      "\n",
      "[4. 3. 3. 3. 3.]\n",
      "\n",
      " 5\n",
      " 4\n",
      " 4\n",
      " 4\n",
      " 4\n",
      "[torch.FloatTensor of size 5]\n",
      "\n",
      "[4. 3. 3. 3. 3.]\n"
     ]
    }
   ],
   "source": [
    "print(a.add_(1))\n",
    "print(b)\n",
    "a[:] += 1\n",
    "print(a)\n",
    "print(b)\n",
    "a = a.add(1)\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each method report the new values of a and b. Whare are the similarities and differences? \n",
    "> a.add_(1): Adds one to all elements in a and b. <br>\n",
    "> a[:] += 1: Adds one to all elements in a and b. <br>\n",
    "> a.add(1): Only adds one to all elements in the torch tensor. The numpy array is unchanged. <br>\n",
    "\n",
    "### 14. Coverting NumPy array to Torch tensors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2. 2. 2. 2. 2.]\n",
      "\n",
      " 2\n",
      " 2\n",
      " 2\n",
      " 2\n",
      " 2\n",
      "[torch.DoubleTensor of size 5]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "a = np.ones(5)\n",
    "b = torch.from_numpy(a)\n",
    "np.add(a, 1, out=a)\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The torch tensor is updated along with the numpy array.\n",
    "### 15. CPU - GPU conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 2.1180  0.0101  0.3099\n",
      "-1.1475 -0.7874  0.5753\n",
      "-1.1571 -0.1085  0.4369\n",
      "-0.2507 -0.4599  1.0138\n",
      "-0.5697 -1.7473 -0.5345\n",
      "[torch.cuda.FloatTensor of size 5x3 (GPU 0)]\n",
      "\n",
      "\n",
      " 2.1180  0.0101  0.3099\n",
      "-1.1475 -0.7874  0.5753\n",
      "-1.1571 -0.1085  0.4369\n",
      "-0.2507 -0.4599  1.0138\n",
      "-0.5697 -1.7473 -0.5345\n",
      "[torch.FloatTensor of size 5x3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(5, 3)\n",
    "y = torch.randn(5, 3)\n",
    "if torch.cuda.is_available():\n",
    "    x = x.cuda()\n",
    "    y = y.cuda()\n",
    "z = x + y\n",
    "print(z)\n",
    "if torch.cuda.is_available():\n",
    "    print(z.cpu())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpret each of these instructions. <br>\n",
    "> x = torch.randn(5, 3): Create random tensor x with size 5x3.<br>\n",
    "> y = torch.randn(5,3): Create random tensor y with size 5x3.<br>\n",
    "> if torch.cuda.is_avalible(): check to see of cuda (GPU) is avalible for use. <br>\n",
    "> x = x.cuda(): Move the torch tensor x to the GPU.<br>\n",
    "> y = y.cuda(): Moved the torch tensor y to the GPU.<br>\n",
    "> z = x + y: Add the two tensors regardless of their memory location (GPU vs CPU)<br>\n",
    "> print(z.cpu()): Move the tensor z to the CPU, print the tensor. <br>\n",
    "\n",
    "Would the program produce the same result if it was run on a CPU only architecture? <br>\n",
    "> YES<br>\n",
    "\n",
    "### 16. Run and interpret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2.1179965   0.01008026  0.30988687]\n",
      " [-1.147473   -0.7874392   0.575319  ]\n",
      " [-1.1571122  -0.1085149   0.43693048]\n",
      " [-0.25067568 -0.4598683   1.0137987 ]\n",
      " [-0.569722   -1.7473488  -0.5345496 ]]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "can't convert CUDA tensor to numpy (it doesn't support GPU arrays). Use .cpu() to move the tensor to host memory first.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-11d5c486e6eb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: can't convert CUDA tensor to numpy (it doesn't support GPU arrays). Use .cpu() to move the tensor to host memory first."
     ]
    }
   ],
   "source": [
    "print(z.cpu().numpy())\n",
    "print(z.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpret...<br>\n",
    "> z.cpu().numpy() moves the data from the GPU back to the CPU where it is then converted into a numpy array.<br>\n",
    "> Runtime error because data in the GPU cannot be converted into a numpy array. <br>\n",
    "\n",
    "## 6 Autograd: automatic differentiation\n",
    "### 17. Autograd intro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 1  1\n",
      " 1  1\n",
      "[torch.FloatTensor of size 2x2]\n",
      "\n",
      "Variable containing:\n",
      " 3  3\n",
      " 3  3\n",
      "[torch.FloatTensor of size 2x2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch.autograd as ag\n",
    "\n",
    "x = ag.Variable(torch.ones(2, 2), requires_grad=True)\n",
    "print(x)\n",
    "y = x + 2\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the type of y?\n",
    "> y is autograd variable containing a torch.FloatTensor with size 2x2\n",
    "\n",
    "### 18. Run the following script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 27  27\n",
      " 27  27\n",
      "[torch.FloatTensor of size 2x2]\n",
      " Variable containing:\n",
      " 27\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "z = y * y * 3\n",
    "f = z.mean()\n",
    "print(z, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write the equation linking f with the four entries of x.\n",
    "$$ f(x)= \\frac{3(x_1+2)^2+3(x_2+2)^2+3(x_3+2)^2+3(x_4+2)^2}{4} $$\n",
    "### 19. Backpropagate the gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 4.5000  4.5000\n",
      " 4.5000  4.5000\n",
      "[torch.FloatTensor of size 2x2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "f.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gradient of f with respect to x is : [[4.5, 4.5][4.5, 4.5]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 20. Derive the partial derivatives for each x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ (\\nabla_x f(\\mathbf{x})))_i = \\frac {\\partial f(x_1, x_2, x_3, x_4)}{\\partial x_i}$$<br>\n",
    "$$ f(\\mathbf{x})= \\frac{3}{4}\\left \\{(x_1+2)^2+(x_2+2)^2+(x_3+2)^2+(x_4+2)^2\\right\\} $$<br>\n",
    "$$ \\frac {\\partial f(\\mathbf{x})}{\\partial x_1} = \\frac{3(2+x_1)}{2}\\bigg\\rvert_{x_1 = 1} = 4.5$$<br>\n",
    "$$ \\frac {\\partial f(\\mathbf{x})}{\\partial x_2} = \\frac{3(2+x_2)}{2}\\bigg\\rvert_{x_2 = 1} = 4.5$$<br>\n",
    "$$ \\frac {\\partial f(\\mathbf{x})}{\\partial x_3} = \\frac{3(2+x_3)}{2}\\bigg\\rvert_{x_3 = 1} = 4.5$$<br>\n",
    "$$ \\frac {\\partial f(\\mathbf{x})}{\\partial x_4} = \\frac{3(2+x_4)}{2}\\bigg\\rvert_{x_4 = 1} = 4.5$$<br>\n",
    "<br>\n",
    "Autograd produces the correct answer.<br>\n",
    "## 7 MNIST Data preparation\n",
    "### 21. Load and normalize MNIST data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import MNISTtools\n",
    "\n",
    "def normalize_MNIST_images(x):\n",
    "    # Normalize images from to [-1, 1]\n",
    "    x = x.astype(np.float64) # convert to float\n",
    "    xmax, xmin = x.max(), x.min()\n",
    "    x = 2 * (x - xmin)/(xmax - xmin) - 1\n",
    "    return x\n",
    "\n",
    "xtrain, ltrain = MNISTtools.load(dataset = \"training\", path = \"/datasets/MNIST\")\n",
    "xtrain = normalize_MNIST_images(xtrain)\n",
    "xtest, ltest = MNISTtools.load(dataset = \"testing\", path = \"/datasets/MNIST\")\n",
    "xtest = normalize_MNIST_images(xtest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 22. Reshape data for torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain = xtrain.reshape(28,28,1,60000)\n",
    "xtest = xtest.reshape(28,28,1,10000)\n",
    "\n",
    "xtrain = np.moveaxis(xtrain, [-1, -2] , [0,1])\n",
    "xtest = np.moveaxis(xtest, [-1, -2] , [0,1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 23. Check data reorganization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    }
   ],
   "source": [
    "import matplotlib as pyplot\n",
    "%matplotlib inline  \n",
    "MNISTtools.show(xtrain[42, 0, :, :])\n",
    "print(ltrain[42])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 24. Convert into autograd variables (modified in 31)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    xtrain = ag.Variable(torch.from_numpy(xtrain).cuda(), requires_grad=True)\n",
    "    ltrain = ag.Variable(torch.from_numpy(ltrain).cuda(), requires_grad=False)\n",
    "    xtest = ag.Variable(torch.from_numpy(xtest).cuda(), requires_grad=False)\n",
    "    ltest = ag.Variable(torch.from_numpy(ltest).cuda(), requires_grad=False)\n",
    "\n",
    "\n",
    "else:\n",
    "    xtrain = ag.Variable(torch.from_numpy(xtrain), requires_grad=True)\n",
    "    ltrain = ag.Variable(torch.from_numpy(ltrain), requires_grad=False)\n",
    "    xtest = ag.Variable(torch.from_numpy(xtest), requires_grad=False)\n",
    "    ltest = ag.Variable(torch.from_numpy(ltest), requires_grad=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8 CNN for MNIST Classification\n",
    "### 25.LeNet\n",
    "Determine the size of the feature maps after each convolution and maxpooling operation. <br>\n",
    "> INPUT: 28x28<br>\n",
    "> CONV1: [28-5+1],[28-5+1] = 24x24<br>\n",
    "> POOL1: 12x12<br>\n",
    "> CONV2: 8x8<br>\n",
    "> POOL2: 4x4<br>\n",
    "\n",
    "How many input units are there in the third layer?<br>\n",
    "> There are 256 output channels at the third layer.<br>\n",
    "\n",
    "### 26. Initialize LeNet Network (modified in 31)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LeNet(\n",
      "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=256, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#This is our neural network class that inherits from nn.Module\n",
    "class LeNet(nn.Module):\n",
    "    \n",
    "    #Here we define our network structure\n",
    "    def __init__(self):\n",
    "        super(LeNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5).double()\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5).double()\n",
    "        self.fc1 = nn.Linear(16*4*4, 120).double()\n",
    "        self.fc2 = nn.Linear(120, 84).double()\n",
    "        self.fc3 = nn.Linear(84, 10).double()\n",
    "        \n",
    "    #Here we define one forward pass through the network\n",
    "    def forward(self, x):\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2,2))\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), (2,2))\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        x = F.relu(self.fc1(x)) \n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "    #determine the number of features in a batch of tensors\n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]\n",
    "        return np.prod(size)\n",
    "    \n",
    "if torch.cuda.is_available():\n",
    "    net = LeNet().cuda()\n",
    "else:\n",
    "    net = LeNet()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 27. Learnable parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 torch.Size([6, 1, 5, 5])\n",
      "1 torch.Size([6])\n",
      "2 torch.Size([16, 6, 5, 5])\n",
      "3 torch.Size([16])\n",
      "4 torch.Size([120, 256])\n",
      "5 torch.Size([120])\n",
      "6 torch.Size([84, 120])\n",
      "7 torch.Size([84])\n",
      "8 torch.Size([10, 84])\n",
      "9 torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "params = list(net.parameters())\n",
    "for i in range(len(params)):\n",
    "    print(i, params[i].size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the parameters corresponding to indicies 0, 2, 4, 6, 8? Odd indices? \n",
    "> The parameters at indicies 0 and 2 correspond to: the number of output features, number of input features, kernel width, and kernel height for the convolution layers.<br>\n",
    "> The parameters at indicies 4,6, and 8 correspond to: the number of input features and the number of output features for the fully connected layers.<br>\n",
    "> The parameters at the odd indices correspond to the number of connections between the layers.<br>\n",
    "\n",
    "### 28. Forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "eq() received an invalid combination of arguments - got (numpy.ndarray), but expected one of:\n * (float other)\n      didn't match because some of the arguments have invalid types: (\u001b[31;1mnumpy.ndarray\u001b[0m)\n * (Variable other)\n      didn't match because some of the arguments have invalid types: (\u001b[31;1mnumpy.ndarray\u001b[0m)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-84-f2405e46490b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0myinit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mltest\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0myinit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: eq() received an invalid combination of arguments - got (numpy.ndarray), but expected one of:\n * (float other)\n      didn't match because some of the arguments have invalid types: (\u001b[31;1mnumpy.ndarray\u001b[0m)\n * (Variable other)\n      didn't match because some of the arguments have invalid types: (\u001b[31;1mnumpy.ndarray\u001b[0m)\n"
     ]
    }
   ],
   "source": [
    "yinit = net(xtest)\n",
    "print(100 * np.mean(ltest == yinit.cpu().data.numpy().T.argmax(axis=0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpret the result...<br>\n",
    "> On average the output is equal to the test label ~10% of the time. This is resonable since there are 10 output classes (0-9).<br>\n",
    "\n",
    "### 29. Minibatch SGD w/ cross-entropy and momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = xtrain.size()[0] # training set size\n",
    "B = 100 # Minibatch size\n",
    "NB = N/B #Number of minibatches\n",
    "T = 10 # Number of epochs\n",
    "gamma = 0.001 # learning rate\n",
    "rho = 0.9 # momentum \n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=gamma, momentum=rho)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 30. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   100] loss: 0.02303\n",
      "[1,   200] loss: 0.02297\n",
      "[1,   300] loss: 0.02292\n",
      "[1,   400] loss: 0.02285\n",
      "[1,   500] loss: 0.02278\n",
      "[1,   600] loss: 0.02262\n",
      "[2,   100] loss: 0.02231\n",
      "[2,   200] loss: 0.02159\n",
      "[2,   300] loss: 0.01945\n",
      "[2,   400] loss: 0.01368\n",
      "[2,   500] loss: 0.00746\n",
      "[2,   600] loss: 0.00521\n",
      "[3,   100] loss: 0.00399\n",
      "[3,   200] loss: 0.00369\n",
      "[3,   300] loss: 0.00297\n",
      "[3,   400] loss: 0.00295\n",
      "[3,   500] loss: 0.00259\n",
      "[3,   600] loss: 0.00230\n",
      "[4,   100] loss: 0.00223\n",
      "[4,   200] loss: 0.00209\n",
      "[4,   300] loss: 0.00193\n",
      "[4,   400] loss: 0.00206\n",
      "[4,   500] loss: 0.00181\n",
      "[4,   600] loss: 0.00169\n",
      "[5,   100] loss: 0.00155\n",
      "[5,   200] loss: 0.00165\n",
      "[5,   300] loss: 0.00159\n",
      "[5,   400] loss: 0.00141\n",
      "[5,   500] loss: 0.00142\n",
      "[5,   600] loss: 0.00139\n",
      "[6,   100] loss: 0.00141\n",
      "[6,   200] loss: 0.00124\n",
      "[6,   300] loss: 0.00130\n",
      "[6,   400] loss: 0.00119\n",
      "[6,   500] loss: 0.00114\n",
      "[6,   600] loss: 0.00113\n",
      "[7,   100] loss: 0.00113\n",
      "[7,   200] loss: 0.00107\n",
      "[7,   300] loss: 0.00110\n",
      "[7,   400] loss: 0.00097\n",
      "[7,   500] loss: 0.00100\n",
      "[7,   600] loss: 0.00110\n",
      "[8,   100] loss: 0.00108\n",
      "[8,   200] loss: 0.00091\n",
      "[8,   300] loss: 0.00097\n",
      "[8,   400] loss: 0.00096\n",
      "[8,   500] loss: 0.00081\n",
      "[8,   600] loss: 0.00091\n",
      "[9,   100] loss: 0.00096\n",
      "[9,   200] loss: 0.00082\n",
      "[9,   300] loss: 0.00073\n",
      "[9,   400] loss: 0.00091\n",
      "[9,   500] loss: 0.00082\n",
      "[9,   600] loss: 0.00094\n",
      "[10,   100] loss: 0.00083\n",
      "[10,   200] loss: 0.00073\n",
      "[10,   300] loss: 0.00073\n",
      "[10,   400] loss: 0.00081\n",
      "[10,   500] loss: 0.00075\n",
      "[10,   600] loss: 0.00084\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(T):\n",
    "    running_loss = 0.0\n",
    "    idxminibatches = np.random.permutation(NB)  # shuffling\n",
    "    for k in range(NB):\n",
    "        i = idxminibatches[k]  # index of the minibatch\n",
    "\n",
    "        # Extract the i-th minibatch from xtrain and ltrain\n",
    "        idxsmp = range(i*B, (i*B)+B)  # indicies of the sample for the i-th minibatch\n",
    "        inputs = xtrain[idxsmp,:,:,:]\n",
    "        labels = ltrain[idxsmp]\n",
    "\n",
    "        # Initialize the gradients to zero\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward propagation\n",
    "        outputs = net(inputs)\n",
    "\n",
    "        # Error evaluation\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Back propagation\n",
    "        loss.backward()\n",
    "\n",
    "        # Parameter update\n",
    "        optimizer.step()\n",
    "\n",
    "        #Print averaged loss per minibatch every 100 minibatches\n",
    "        running_loss += loss[0]\n",
    "        if k % 100 == 99:\n",
    "            print('[%d, %5d] loss: %.5f' % (epoch + 1, k + 1, running_loss/(B*100)))\n",
    "            running_loss = 0.0\n",
    "print(\"Finished Training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 32. Re-evaluate your predictions of your trained network on the teting dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   100] loss: 0.00071\n",
      "[2,   100] loss: 0.00066\n",
      "[3,   100] loss: 0.00061\n",
      "[4,   100] loss: 0.00058\n",
      "[5,   100] loss: 0.00059\n",
      "[6,   100] loss: 0.00053\n",
      "[7,   100] loss: 0.00053\n",
      "[8,   100] loss: 0.00049\n",
      "[9,   100] loss: 0.00049\n",
      "[10,   100] loss: 0.00046\n",
      "Finished Testing\n"
     ]
    }
   ],
   "source": [
    "N = xtest.size()[0] # training set size\n",
    "B = 100 # Minibatch size\n",
    "NB = N/B #Number of minibatches\n",
    "T = 10 # Number of epochs\n",
    "gamma = 0.001 # learning rate\n",
    "rho = 0.9 # momentum \n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=gamma, momentum=rho)\n",
    "\n",
    "\n",
    "for epoch in range(T):\n",
    "    running_loss = 0.0\n",
    "    idxminibatches = np.random.permutation(NB)  # shuffling\n",
    "    for k in range(NB):\n",
    "        i = idxminibatches[k]  # index of the minibatch\n",
    "\n",
    "        # Extract the i-th minibatch from xtrain and ltrain\n",
    "        idxsmp = range(i*B, (i*B)+B)  # indicies of the sample for the i-th minibatch\n",
    "        inputs = xtest[idxsmp,:,:,:]\n",
    "        labels = ltest[idxsmp]\n",
    "\n",
    "        # Initialize the gradients to zero\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward propagation\n",
    "        outputs = net(inputs)\n",
    "\n",
    "        # Error evaluation\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Back propagation\n",
    "        loss.backward()\n",
    "\n",
    "        # Parameter update\n",
    "        optimizer.step()\n",
    "\n",
    "        #Print averaged loss per minibatch every 100 minibatches\n",
    "        running_loss += loss[0]\n",
    "        if k % 100 == 99:\n",
    "            print('[%d, %5d] loss: %.5f' % (epoch + 1, k + 1, running_loss/(B*100)))\n",
    "            running_loss = 0.0\n",
    "print(\"Finished Testing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How much did the accuracy improve?\n",
    "> Running the training dataset on the GPU leads to a loss of 0.00084 after 10 epochs of training.<br>\n",
    "> Running the testing dataset on the GPU leads to a loss of 0.00046. This is a improvement over the loss on the training dataset indicates that the network has not been overfit to the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
