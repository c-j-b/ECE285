{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment #2 Backpropogation \n",
    "## 1. Getting Started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Read MNIST Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import MNISTtools\n",
    "# help(MNISTtools.load)\n",
    "# help(MNISTtools.show)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 60000)\n",
      "(60000,)\n"
     ]
    }
   ],
   "source": [
    "xtrain, ltrain = MNISTtools.load(dataset = \"training\", path = \"/datasets/MNIST\")\n",
    "print(xtrain.shape)\n",
    "print(ltrain.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1\n",
    "What are the shapes of both variables? <br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;xtrain - (784, 60000), ltrain - (60000, 0)<br>\n",
    "Whare is the size of the training dataset?<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;60,000 images<br>\n",
    "What is the feature dimension?<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;784"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "MNISTtools.show(xtrain[:, 42])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "('Min: ', 0, 'Max: ', 255)\n"
     ]
    }
   ],
   "source": [
    "print(ltrain[42])\n",
    "print('Min: ', np.amin(xtrain), 'Max: ', np.amax(xtrain))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3\n",
    "What is the range of xtrain(min and max values)?<br> \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;0 - 255<br>\n",
    "What is the type of xtrain?<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; xtrain is a numpy.ndarray\n",
    "### 2.4 Normalize Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_MNIST_images(x):\n",
    "    # Normalize images from to [-1, 1]\n",
    "    x = x.astype(np.float64) # convert to float\n",
    "    xmax, xmin = x.max(), x.min()\n",
    "    x = 2 * (x - xmin)/(xmax - xmin) - 1\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check normalize_MNIST_images()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Min: ', -1.0, 'Max: ', 1.0)\n",
      "(784, 60000)\n"
     ]
    }
   ],
   "source": [
    "xtrain = normalize_MNIST_images(xtrain)\n",
    "print('Min: ', xtrain.min(), 'Max: ', xtrain.max()) \n",
    "print(xtrain.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5  Convert labels to oneHot codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label2onehot(lbl):\n",
    "    d = np.zeros((lbl.max() + 1, lbl.size)) \n",
    "    d[lbl, np.arange(0, lbl.size)] = 1 \n",
    "    return d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check label conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 60000)\n",
      "[0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "dtrain = label2onehot(ltrain)\n",
    "print(dtrain.shape)\n",
    "print(dtrain[:, 42])\n",
    "print (ltrain[42])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The one hot label of image 42, [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.], is the correct binary vector representation of label 7."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Convert oneHot codes to labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onehot2label(d):\n",
    "    lbl = d.argmax(axis=0) # need the max of each row\n",
    "    return lbl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check oneHot conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One hot encoding and decoding successful\n"
     ]
    }
   ],
   "source": [
    "if np.array_equal(ltrain,onehot2label(dtrain)):\n",
    "    print('One hot encoding and decoding successful')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Activation functions\n",
    "### 3.7 Numerically safe softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(a):\n",
    "    # Compute softmax\n",
    "    ga = np.exp(a - a.max(axis=0))\n",
    "    return ga / ga.sum(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.21194156 0.10650698 0.21194156 0.33333333]\n",
      " [0.57611688 0.78698604 0.57611688 0.33333333]\n",
      " [0.21194156 0.10650698 0.21194156 0.33333333]]\n"
     ]
    }
   ],
   "source": [
    "x2 = np.array([[1, 2, 1, 6],  # sample 1\n",
    "               [2, 4, 2, 6],  # sample 2\n",
    "               [1, 2, 1, 6]]) # sample 1 again(!)\n",
    "print(softmax(x2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.8 Softmax derivative if i = j\n",
    "\\begin{align}\n",
    "{g(a)_i} & = \\frac{\\exp({a_i})}{\\sum_{j=1}^n\\exp({a_j})} \\\\\n",
    "Using\\space quotient\\space rule \\space and\\space simplifying... \\\\\n",
    "\\frac{\\partial{g(a)_i}}{\\partial{a_j}}& = \\frac{\\exp({a_i})\\sum_{k=1}^n(\\exp({a_k})-\\exp({a_j}))}\n",
    "{\\sum_{k=1}^n\\exp({a_k})} \\\\\n",
    "& = \\frac{\\exp({a_j})}{\\sum_{k=1}^n\\exp({a_k})} x \n",
    "\\frac{\\sum_{k=1}^n(\\exp({a_k}) - \\exp({a_j}))}{\\sum_{k=1}^n\\exp({a_k})} \\\\\n",
    "& = {p_i}(1 - {p_j})\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.9 Softmax derivative if i != j"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "\\frac{\\partial{g(a)_i}}{\\partial{a_j}}& = \\frac{0 - \\exp({a_i})\\exp({a_j})}\n",
    "{(\\sum_{k=1}^n\\exp({a_k}))^2} \\\\\n",
    "& = \\frac{- \\exp({a_j})}{\\sum_{k=1}^n\\exp({a_k})} x \\frac{\\exp({a_i})}{\\sum_{k=1}^n\\exp({a_k})} \\\\\n",
    "& = -{p_j}{p_i}\n",
    "\\end{align}\n",
    "### 3.10 softmaxp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmaxp(a, e):\n",
    "    ga = softmax(a)\n",
    "    prod = ga * e\n",
    "    dot = (ga * e).sum(axis=0)\n",
    "    return prod - dot.T * ga"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.11 Checking spftmaxp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5.048422830729758e-07, ' should be smaller than 1e-6')\n"
     ]
    }
   ],
   "source": [
    "eps = 1e-6 #finite difference step\n",
    "a = np.random.randn(10, 200) #random inputs\n",
    "e = np.random.randn(10, 200) # random directions\n",
    "diff = softmaxp(a, e)\n",
    "a_soft = softmax(a)\n",
    "diff_approx = (softmax(a + e * eps) - a_soft) / eps\n",
    "rel_error = np.abs(diff - diff_approx).mean() / np.abs(diff_approx).mean()\n",
    "print(rel_error, ' should be smaller than 1e-6')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.12 ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(a):\n",
    "    return a * (a > 0) # for speed\n",
    "def relup(a, e):\n",
    "    return (1 * (a > 0)) * e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Backpropagation\n",
    "### 4.13 Network init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_shallow(Ni, Nh, No):\n",
    "    b1 = np.random.randn(Nh, 1) / np.sqrt((Ni+1.)/2.) \n",
    "    W1 = np.random.randn(Nh, Ni) / np.sqrt((Ni+1.)/2.)\n",
    "    b2 = np.random.randn(No, 1) / np.sqrt((Nh+1.)) \n",
    "    W2 = np.random.randn(No, Nh) / np.sqrt((Nh+1.)) \n",
    "    return W1, b1, W2, b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ni = xtrain.shape[0]\n",
    "Nh = 64\n",
    "No = dtrain.shape[0]\n",
    "netinit = init_shallow(Ni, Nh, No)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.14  Forward prop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forwardprop_shallow(x, net):\n",
    "    W1 = net[0]\n",
    "    b1 = net[1]\n",
    "    W2 = net[2]\n",
    "    b2 = net[3]\n",
    "    \n",
    "    a1 = W1.dot(x) + b1\n",
    "    h1 = relu(a1)\n",
    "    a2 = W2.dot(h1) + b2\n",
    "    y = softmax(a2)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 60000)\n"
     ]
    }
   ],
   "source": [
    "yinit = forwardprop_shallow(xtrain, netinit)\n",
    "print(yinit.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.15 eval loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_loss(y, d):\n",
    "    result = -(d * np.log(y)).sum()/y.size\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.25445753553865325, 'should be around 0.26')\n"
     ]
    }
   ],
   "source": [
    "print(eval_loss(yinit, dtrain), 'should be around 0.26')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.16 Eval perfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.889383333333\n"
     ]
    }
   ],
   "source": [
    "def eval_perfs(y, lbl):\n",
    "    y[y.argmax(axis=0), np.arange(0, y.shape[1])] = 1\n",
    "    y[y < 1] = 0\n",
    "    predicted_labels = onehot2label(y)\n",
    "    return 1 - float(np.sum(predicted_labels == lbl))/float(y.shape[1]) \n",
    "print(eval_perfs(yinit, ltrain))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initially the shallow net shows very poor performance.\n",
    "### 4.17 Update shallow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_shallow(x, d, net, gamma=0.05):\n",
    "    W1 = net[0]\n",
    "    b1 = net[1]\n",
    "    W2 = net[2]\n",
    "    b2 = net[3]\n",
    "    Ni = W1.shape[1]\n",
    "    Nh = W1.shape[0]\n",
    "    No = W2.shape[0]\n",
    "    \n",
    "    gamma = gamma / x.shape[1] # normalized by the training dataset size\n",
    "    \n",
    "    # forward phase\n",
    "    a1 = W1.dot(x) + b1 # [10x60k]\n",
    "    h1 = relu(a1)\n",
    "    a2 = W2.dot(h1) + b2 # [64x60k]\n",
    "    y = softmax(a2)\n",
    "    \n",
    "    # error\n",
    "    e = eval_loss(y, d)\n",
    "\n",
    "    # backward phase\n",
    "    delta2 = softmaxp(a2, -d/y)\n",
    "    delta1 = relup(a1, W2.T.dot(delta2))\n",
    "    \n",
    "    # gradient update\n",
    "    W2 = W2 - gamma * delta2.dot(h1.T)\n",
    "    W1 = W1 - gamma * delta1.dot(x.T)\n",
    "    b2 = b2 - gamma * delta2.sum(axis=1).reshape(No, 1)\n",
    "    b1 = b1 - gamma * delta1.sum(axis=1).reshape(Nh, 1)\n",
    "    return W1, b1, W2, b2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.18 Backprop shallow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backprop_shallow(x, d, net, T, gamma=.05): \n",
    "    lbl = onehot2label(d)\n",
    "    for t in range(0, T):\n",
    "        net = update_shallow(x, d, net, gamma)\n",
    "        y = forwardprop_shallow(x, net)\n",
    "        if t % 5 == 0:\n",
    "            print('T: ', t, 'Loss: ', eval_loss(y, d), 'Error: ', eval_perfs(y, lbl))\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('T: ', 0, 'Loss: ', 0.23502028504254116, 'Perfs: ', 0.8597666666666667)\n",
      "('T: ', 1, 'Loss: ', 0.22191315018550548, 'Perfs: ', 0.79625)\n",
      "('T: ', 2, 'Loss: ', 0.2133725449748527, 'Perfs: ', 0.7170000000000001)\n",
      "('T: ', 3, 'Loss: ', 0.20592179394480806, 'Perfs: ', 0.6618999999999999)\n",
      "('T: ', 4, 'Loss: ', 0.19885955796734722, 'Perfs: ', 0.61605)\n",
      "('T: ', 5, 'Loss: ', 0.1921007871656288, 'Perfs: ', 0.5789833333333334)\n",
      "('T: ', 6, 'Loss: ', 0.18553236104294654, 'Perfs: ', 0.5438833333333333)\n",
      "('T: ', 7, 'Loss: ', 0.17910571722706264, 'Perfs: ', 0.5125500000000001)\n",
      "('T: ', 8, 'Loss: ', 0.17282287408678673, 'Perfs: ', 0.4824166666666667)\n",
      "('T: ', 9, 'Loss: ', 0.16675339508526865, 'Perfs: ', 0.4548)\n",
      "('T: ', 10, 'Loss: ', 0.16094753133534162, 'Perfs: ', 0.43115000000000003)\n",
      "('T: ', 11, 'Loss: ', 0.1554552467748824, 'Perfs: ', 0.4072)\n",
      "('T: ', 12, 'Loss: ', 0.15025994391361944, 'Perfs: ', 0.3899)\n",
      "('T: ', 13, 'Loss: ', 0.14535652582947692, 'Perfs: ', 0.36998333333333333)\n",
      "('T: ', 14, 'Loss: ', 0.14074233980797343, 'Perfs: ', 0.3566666666666667)\n",
      "('T: ', 15, 'Loss: ', 0.13646513901009508, 'Perfs: ', 0.3403666666666667)\n",
      "('T: ', 16, 'Loss: ', 0.13260197659846357, 'Perfs: ', 0.3332166666666667)\n",
      "('T: ', 17, 'Loss: ', 0.12936805700079157, 'Perfs: ', 0.32594999999999996)\n",
      "('T: ', 18, 'Loss: ', 0.1270066584671902, 'Perfs: ', 0.3292333333333334)\n",
      "('T: ', 19, 'Loss: ', 0.12622590096076014, 'Perfs: ', 0.34578333333333333)\n",
      "('T: ', 20, 'Loss: ', 0.12651650687909993, 'Perfs: ', 0.3635333333333334)\n",
      "('T: ', 21, 'Loss: ', 0.12865063544740613, 'Perfs: ', 0.3887333333333334)\n",
      "('T: ', 22, 'Loss: ', 0.12682091681569682, 'Perfs: ', 0.3845166666666666)\n",
      "('T: ', 23, 'Loss: ', 0.12432562958192948, 'Perfs: ', 0.3763333333333333)\n",
      "('T: ', 24, 'Loss: ', 0.11759409645270537, 'Perfs: ', 0.34548333333333336)\n",
      "('T: ', 25, 'Loss: ', 0.11310442724952516, 'Perfs: ', 0.3234)\n",
      "('T: ', 26, 'Loss: ', 0.10814281243167781, 'Perfs: ', 0.2987833333333333)\n",
      "('T: ', 27, 'Loss: ', 0.1049894472329015, 'Perfs: ', 0.2856833333333333)\n",
      "('T: ', 28, 'Loss: ', 0.10166770039272807, 'Perfs: ', 0.26949999999999996)\n",
      "('T: ', 29, 'Loss: ', 0.09921659462203274, 'Perfs: ', 0.2612833333333333)\n",
      "('T: ', 30, 'Loss: ', 0.09665451502450025, 'Perfs: ', 0.2505166666666667)\n",
      "('T: ', 31, 'Loss: ', 0.09461878618881225, 'Perfs: ', 0.24519999999999997)\n",
      "('T: ', 32, 'Loss: ', 0.0924367765449586, 'Perfs: ', 0.2362833333333333)\n",
      "('T: ', 33, 'Loss: ', 0.09066553781592765, 'Perfs: ', 0.2322333333333333)\n",
      "('T: ', 34, 'Loss: ', 0.08870355330662706, 'Perfs: ', 0.22465000000000002)\n",
      "('T: ', 35, 'Loss: ', 0.0871112090686224, 'Perfs: ', 0.22243333333333337)\n",
      "('T: ', 36, 'Loss: ', 0.08531762430333703, 'Perfs: ', 0.2140833333333333)\n",
      "('T: ', 37, 'Loss: ', 0.08387105789080154, 'Perfs: ', 0.21256666666666668)\n",
      "('T: ', 38, 'Loss: ', 0.0822303018884441, 'Perfs: ', 0.20499999999999996)\n",
      "('T: ', 39, 'Loss: ', 0.08091447049059203, 'Perfs: ', 0.20365)\n",
      "('T: ', 40, 'Loss: ', 0.07941564821139072, 'Perfs: ', 0.19751666666666667)\n",
      "('T: ', 41, 'Loss: ', 0.07822142185281676, 'Perfs: ', 0.19676666666666665)\n",
      "('T: ', 42, 'Loss: ', 0.076865888330982, 'Perfs: ', 0.1904)\n",
      "('T: ', 43, 'Loss: ', 0.07578005086160906, 'Perfs: ', 0.19058333333333333)\n",
      "('T: ', 44, 'Loss: ', 0.07455500613384354, 'Perfs: ', 0.18389999999999995)\n",
      "('T: ', 45, 'Loss: ', 0.07356099991299782, 'Perfs: ', 0.18423333333333336)\n",
      "('T: ', 46, 'Loss: ', 0.07245777396696144, 'Perfs: ', 0.17958333333333332)\n",
      "('T: ', 47, 'Loss: ', 0.07154619988481756, 'Perfs: ', 0.17898333333333338)\n",
      "('T: ', 48, 'Loss: ', 0.07055106451006145, 'Perfs: ', 0.17476666666666663)\n",
      "('T: ', 49, 'Loss: ', 0.06971337786202166, 'Perfs: ', 0.17471666666666663)\n",
      "('T: ', 50, 'Loss: ', 0.06881618358323073, 'Perfs: ', 0.1706833333333333)\n",
      "('T: ', 51, 'Loss: ', 0.06804590558817025, 'Perfs: ', 0.17066666666666663)\n",
      "('T: ', 52, 'Loss: ', 0.0672340677098609, 'Perfs: ', 0.16700000000000004)\n",
      "('T: ', 53, 'Loss: ', 0.066521912458631, 'Perfs: ', 0.16735)\n",
      "('T: ', 54, 'Loss: ', 0.06578303810438042, 'Perfs: ', 0.1640666666666667)\n",
      "('T: ', 55, 'Loss: ', 0.06512112331388438, 'Perfs: ', 0.16386666666666672)\n",
      "('T: ', 56, 'Loss: ', 0.06444751485282489, 'Perfs: ', 0.16083333333333338)\n",
      "('T: ', 57, 'Loss: ', 0.06383242783585232, 'Perfs: ', 0.16095000000000004)\n",
      "('T: ', 58, 'Loss: ', 0.0632136591856101, 'Perfs: ', 0.15816666666666668)\n",
      "('T: ', 59, 'Loss: ', 0.06264018537373707, 'Perfs: ', 0.1586333333333333)\n",
      "('T: ', 60, 'Loss: ', 0.06206904879623385, 'Perfs: ', 0.15568333333333328)\n",
      "('T: ', 61, 'Loss: ', 0.06153348494489578, 'Perfs: ', 0.15659999999999996)\n",
      "('T: ', 62, 'Loss: ', 0.061005191759973376, 'Perfs: ', 0.15393333333333337)\n",
      "('T: ', 63, 'Loss: ', 0.06050436811720023, 'Perfs: ', 0.15451666666666664)\n",
      "('T: ', 64, 'Loss: ', 0.06001227236676298, 'Perfs: ', 0.1519666666666667)\n",
      "('T: ', 65, 'Loss: ', 0.05954217320357331, 'Perfs: ', 0.15238333333333332)\n",
      "('T: ', 66, 'Loss: ', 0.05908231913843004, 'Perfs: ', 0.15033333333333332)\n",
      "('T: ', 67, 'Loss: ', 0.058640494971771984, 'Perfs: ', 0.15026666666666666)\n",
      "('T: ', 68, 'Loss: ', 0.05820870608845471, 'Perfs: ', 0.14895000000000003)\n",
      "('T: ', 69, 'Loss: ', 0.0577920993429617, 'Perfs: ', 0.1483833333333333)\n",
      "('T: ', 70, 'Loss: ', 0.05738525245979305, 'Perfs: ', 0.14718333333333333)\n",
      "('T: ', 71, 'Loss: ', 0.05699147527358343, 'Perfs: ', 0.14680000000000004)\n",
      "('T: ', 72, 'Loss: ', 0.056607379120366716, 'Perfs: ', 0.1453833333333333)\n",
      "('T: ', 73, 'Loss: ', 0.05623472808278253, 'Perfs: ', 0.1455333333333333)\n",
      "('T: ', 74, 'Loss: ', 0.05587156777228119, 'Perfs: ', 0.14405)\n",
      "('T: ', 75, 'Loss: ', 0.05551859299735404, 'Perfs: ', 0.14403333333333335)\n",
      "('T: ', 76, 'Loss: ', 0.05517428684301749, 'Perfs: ', 0.1429166666666667)\n",
      "('T: ', 77, 'Loss: ', 0.05483917892646436, 'Perfs: ', 0.14249999999999996)\n",
      "('T: ', 78, 'Loss: ', 0.054512361360684394, 'Perfs: ', 0.14173333333333338)\n",
      "('T: ', 79, 'Loss: ', 0.05419387092736252, 'Perfs: ', 0.1414333333333333)\n",
      "('T: ', 80, 'Loss: ', 0.05388302003839787, 'Perfs: ', 0.1407666666666667)\n",
      "('T: ', 81, 'Loss: ', 0.05358003982616532, 'Perfs: ', 0.1403333333333333)\n",
      "('T: ', 82, 'Loss: ', 0.05328409967786479, 'Perfs: ', 0.13980000000000004)\n",
      "('T: ', 83, 'Loss: ', 0.05299515889214549, 'Perfs: ', 0.1392)\n",
      "('T: ', 84, 'Loss: ', 0.05271262980096593, 'Perfs: ', 0.1387666666666667)\n",
      "('T: ', 85, 'Loss: ', 0.05243661239633297, 'Perfs: ', 0.13841666666666663)\n",
      "('T: ', 86, 'Loss: ', 0.052166835613893785, 'Perfs: ', 0.1377166666666667)\n",
      "('T: ', 87, 'Loss: ', 0.051903164095035405, 'Perfs: ', 0.13706666666666667)\n",
      "('T: ', 88, 'Loss: ', 0.05164529893943028, 'Perfs: ', 0.13661666666666672)\n",
      "('T: ', 89, 'Loss: ', 0.051393204325180186, 'Perfs: ', 0.13603333333333334)\n",
      "('T: ', 90, 'Loss: ', 0.0511464654864405, 'Perfs: ', 0.13570000000000004)\n",
      "('T: ', 91, 'Loss: ', 0.050905035150838225, 'Perfs: ', 0.13518333333333332)\n",
      "('T: ', 92, 'Loss: ', 0.050668682699728386, 'Perfs: ', 0.13475000000000004)\n",
      "('T: ', 93, 'Loss: ', 0.05043738915666473, 'Perfs: ', 0.13434999999999997)\n",
      "('T: ', 94, 'Loss: ', 0.05021084829167061, 'Perfs: ', 0.1338166666666667)\n",
      "('T: ', 95, 'Loss: ', 0.04998904137278363, 'Perfs: ', 0.13336666666666663)\n",
      "('T: ', 96, 'Loss: ', 0.04977181918080626, 'Perfs: ', 0.133)\n",
      "('T: ', 97, 'Loss: ', 0.049558982197726684, 'Perfs: ', 0.13275000000000003)\n",
      "('T: ', 98, 'Loss: ', 0.04935034562254491, 'Perfs: ', 0.13208333333333333)\n",
      "('T: ', 99, 'Loss: ', 0.04914585934387999, 'Perfs: ', 0.13185000000000002)\n"
     ]
    }
   ],
   "source": [
    "nettrain = backprop_shallow(xtrain, dtrain, netinit, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With 100 iterations The error reaches 13.1% and the loss is 0.049.\n",
    "### 4.19 Testing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((784, 10000), (10000,))\n"
     ]
    }
   ],
   "source": [
    "xtest, ltest = MNISTtools.load(dataset = \"testing\", path = \"/datasets/MNIST\")\n",
    "print(xtest.shape, ltest.shape)\n",
    "xtest = normalize_MNIST_images(xtest)\n",
    "dtest = label2onehot(ltest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The testing dataset has 10,000 images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('T: ', 0, 'Loss: ', 0.04638367067298492, 'Error: ', 0.123)\n",
      "('T: ', 5, 'Loss: ', 0.045225261173635956, 'Error: ', 0.12090000000000001)\n",
      "('T: ', 10, 'Loss: ', 0.04420500707203926, 'Error: ', 0.11829999999999996)\n",
      "('T: ', 15, 'Loss: ', 0.04328932265501037, 'Error: ', 0.11619999999999997)\n",
      "('T: ', 20, 'Loss: ', 0.042459733558906886, 'Error: ', 0.11470000000000002)\n",
      "('T: ', 25, 'Loss: ', 0.04170234504843045, 'Error: ', 0.11229999999999996)\n",
      "('T: ', 30, 'Loss: ', 0.04100489099865156, 'Error: ', 0.11070000000000002)\n",
      "('T: ', 35, 'Loss: ', 0.040359409451545765, 'Error: ', 0.10929999999999995)\n",
      "('T: ', 40, 'Loss: ', 0.03975975912818162, 'Error: ', 0.10770000000000002)\n",
      "('T: ', 45, 'Loss: ', 0.039201119437916765, 'Error: ', 0.10609999999999997)\n",
      "('T: ', 50, 'Loss: ', 0.03867729519928849, 'Error: ', 0.10519999999999996)\n",
      "('T: ', 55, 'Loss: ', 0.03818526603348765, 'Error: ', 0.10450000000000004)\n",
      "('T: ', 60, 'Loss: ', 0.0377213584864051, 'Error: ', 0.1038)\n",
      "('T: ', 65, 'Loss: ', 0.03728282751217023, 'Error: ', 0.10350000000000004)\n",
      "('T: ', 70, 'Loss: ', 0.036867929726075666, 'Error: ', 0.10250000000000004)\n",
      "('T: ', 75, 'Loss: ', 0.03647459131458162, 'Error: ', 0.10119999999999996)\n",
      "('T: ', 80, 'Loss: ', 0.0361009215345766, 'Error: ', 0.1008)\n",
      "('T: ', 85, 'Loss: ', 0.035743700960981915, 'Error: ', 0.09999999999999998)\n",
      "('T: ', 90, 'Loss: ', 0.03540279877593901, 'Error: ', 0.09850000000000003)\n",
      "('T: ', 95, 'Loss: ', 0.035077437827407014, 'Error: ', 0.09840000000000004)\n"
     ]
    }
   ],
   "source": [
    "nettest = backprop_shallow(xtest, dtest, nettrain, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The trained network had much better performance and ran much faster. The error after 100 iterations was 9% and the loss was 0.034. \n",
    "### 4.20 Minibatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('T: ', 0, 'Loss: ', 0.03529789509462131, 'Error: ', 0.11071666666666669)\n",
      "('T: ', 1, 'Loss: ', 0.026923780882758215, 'Error: ', 0.08261666666666667)\n",
      "('T: ', 2, 'Loss: ', 0.02186364931079035, 'Error: ', 0.06625000000000003)\n",
      "('T: ', 3, 'Loss: ', 0.01912832861209897, 'Error: ', 0.0587833333333333)\n",
      "('T: ', 4, 'Loss: ', 0.016823098752241805, 'Error: ', 0.05123333333333335)\n"
     ]
    }
   ],
   "source": [
    "def backprop_minibatch_shallow(x, d, net, T, B=100, gamma=.05): \n",
    "    N = x.shape[1]\n",
    "    lbl = onehot2label(d)\n",
    "    for t in range(0, T):\n",
    "        for l in range(0, (N+B-1)/B):\n",
    "            idx = np.arange(B*l, min(B*(l+1), N))\n",
    "            net = update_shallow(x[:,idx], d[:,idx], net, gamma)\n",
    "        y = forwardprop_shallow(x, net)\n",
    "        print('T: ', t, 'Loss: ', eval_loss(y, d), 'Error: ', eval_perfs(y, lbl))    \n",
    "    return net\n",
    "netminibatch = backprop_minibatch_shallow(xtrain, dtrain, netinit, 5, B=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.21 Minibatch testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('T: ', 0, 'Loss: ', 0.01480992124528303, 'Error: ', 0.044499999999999984)\n",
      "('T: ', 1, 'Loss: ', 0.013699655423460428, 'Error: ', 0.042200000000000015)\n",
      "('T: ', 2, 'Loss: ', 0.012932930874962036, 'Error: ', 0.03969999999999996)\n",
      "('T: ', 3, 'Loss: ', 0.012188205345256612, 'Error: ', 0.0373)\n",
      "('T: ', 4, 'Loss: ', 0.011522161707503835, 'Error: ', 0.03520000000000001)\n"
     ]
    }
   ],
   "source": [
    "netminibatchtest = backprop_minibatch_shallow(xtest, dtest, netminibatch, 5, B=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The minibatch algorithm is more efficient and has less error. For 5 epochs with minibatch size of 100, the error was 3.5% and the loss was 0.011. This is superior to the standard backprop algorithm.\n",
    "### 4.22 More hidden nodes, minibatch algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('T: ', 0, 'Loss: ', 0.04002498231983609, 'Error: ', 0.12791666666666668)\n",
      "('T: ', 1, 'Loss: ', 0.032066780781495684, 'Error: ', 0.1001333333333333)\n",
      "('T: ', 2, 'Loss: ', 0.02929111365247636, 'Error: ', 0.09145000000000003)\n",
      "('T: ', 3, 'Loss: ', 0.02692495868298967, 'Error: ', 0.0837)\n",
      "('T: ', 4, 'Loss: ', 0.025539702025705695, 'Error: ', 0.07916666666666672)\n"
     ]
    }
   ],
   "source": [
    "netinit16 = init_shallow(Ni, 16, No)\n",
    "nettrain16  = backprop_minibatch_shallow(xtrain, dtrain, netinit16, 5, B=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('T: ', 0, 'Loss: ', 0.024544890735134224, 'Error: ', 0.07563333333333333)\n",
      "('T: ', 1, 'Loss: ', 0.02375407976651502, 'Error: ', 0.07316666666666671)\n",
      "('T: ', 2, 'Loss: ', 0.023269078628798023, 'Error: ', 0.07153333333333334)\n",
      "('T: ', 3, 'Loss: ', 0.022662641515778563, 'Error: ', 0.06963333333333332)\n",
      "('T: ', 4, 'Loss: ', 0.02258204548738588, 'Error: ', 0.06993333333333329)\n"
     ]
    }
   ],
   "source": [
    "nettest16  = backprop_minibatch_shallow(xtrain, dtrain, nettrain16, 5, B=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Increasing the number of hidden units to 16 did not improve the loss or error. At 100 iterations the error was 6.9% and the loss was 0.012."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('T: ', 0, 'Loss: ', 0.030651057480846993, 'Error: ', 0.09550000000000003)\n",
      "('T: ', 1, 'Loss: ', 0.023019820728543514, 'Error: ', 0.0713166666666667)\n",
      "('T: ', 2, 'Loss: ', 0.018792362223676667, 'Error: ', 0.05783333333333329)\n",
      "('T: ', 3, 'Loss: ', 0.015856981211198815, 'Error: ', 0.04795000000000005)\n",
      "('T: ', 4, 'Loss: ', 0.013675140828283574, 'Error: ', 0.04079999999999995)\n"
     ]
    }
   ],
   "source": [
    "netinit256 = init_shallow(Ni, 256, No)\n",
    "nettrain256  = backprop_minibatch_shallow(xtrain, dtrain, netinit256, 5, B=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('T: ', 0, 'Loss: ', 0.012012696133176763, 'Error: ', 0.03598333333333337)\n",
      "('T: ', 1, 'Loss: ', 0.010663653351011835, 'Error: ', 0.03211666666666668)\n",
      "('T: ', 2, 'Loss: ', 0.009620223740400639, 'Error: ', 0.028766666666666718)\n",
      "('T: ', 3, 'Loss: ', 0.008748225762343504, 'Error: ', 0.026116666666666677)\n",
      "('T: ', 4, 'Loss: ', 0.007944758098283487, 'Error: ', 0.02371666666666672)\n"
     ]
    }
   ],
   "source": [
    "nettest256  = backprop_minibatch_shallow(xtrain, dtrain, nettrain256, 5, B=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Increasing the number of hidden units to 256 improved the loss and error. At 100 iterations the error was 2.3% and the loss was 0.007."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
