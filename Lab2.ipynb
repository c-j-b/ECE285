{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment #2 Backpropogation \n",
    "## 1. Getting Started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Read MNIST Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import MNISTtools\n",
    "# help(MNISTtools.load)\n",
    "# help(MNISTtools.show)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 60000)\n",
      "(60000,)\n"
     ]
    }
   ],
   "source": [
    "xtrain, ltrain = MNISTtools.load(dataset = \"training\", path = \"/datasets/MNIST\")\n",
    "print(xtrain.shape)\n",
    "print(ltrain.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1\n",
    "What are the shapes of both variables? <br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;xtrain - (784, 60000), ltrain - (60000, 0)<br>\n",
    "Whare is the size of the training dataset?<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;60,000 images<br>\n",
    "What is the feature dimension?<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;784"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "MNISTtools.show(xtrain[:, 42])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "('Min: ', 0, 'Max: ', 255)\n"
     ]
    }
   ],
   "source": [
    "print(ltrain[42])\n",
    "print('Min: ', np.amin(xtrain), 'Max: ', np.amax(xtrain))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3\n",
    "What is the range of xtrain(min and max values)?<br> \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;0 - 255<br>\n",
    "What is the type of xtrain?<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; xtrain is a numpy.ndarray\n",
    "### 2.4 Normalize Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_MNIST_images(x):\n",
    "    # Normalize images from to [-1, 1]\n",
    "    x = x.astype(np.float64) # convert to float\n",
    "    xmax, xmin = x.max(), x.min()\n",
    "    x = 2 * (x - xmin)/(xmax - xmin) - 1\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check normalize_MNIST_images()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Min: ', -1.0, 'Max: ', 1.0)\n",
      "(784, 60000)\n"
     ]
    }
   ],
   "source": [
    "xtrain = normalize_MNIST_images(xtrain)\n",
    "print('Min: ', xtrain.min(), 'Max: ', xtrain.max()) \n",
    "print(xtrain.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5  Convert labels to oneHot codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label2onehot(lbl):\n",
    "    d = np.zeros((lbl.max() + 1, lbl.size)) \n",
    "    d[lbl, np.arange(0, lbl.size)] = 1 \n",
    "    return d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check label conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 60000)\n",
      "[0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "dtrain = label2onehot(ltrain)\n",
    "print(dtrain.shape)\n",
    "print(dtrain[:, 42])\n",
    "print (ltrain[42])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The one hot label of image 42, [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.], is the correct binary vector representation of label 7."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Convert oneHot codes to labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onehot2label(d):\n",
    "    lbl = d.argmax(axis=0) # need the max of each row\n",
    "    return lbl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check oneHot conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One hot encoding and decoding successful\n"
     ]
    }
   ],
   "source": [
    "if np.array_equal(ltrain,onehot2label(dtrain)):\n",
    "    print('One hot encoding and decoding successful')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Activation functions\n",
    "### 3.7 Numerically safe softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(a):\n",
    "    # Compute softmax\n",
    "    ga = np.exp(a - a.max(axis=0))\n",
    "    return ga / ga.sum(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.21194156 0.10650698 0.21194156 0.33333333]\n",
      " [0.57611688 0.78698604 0.57611688 0.33333333]\n",
      " [0.21194156 0.10650698 0.21194156 0.33333333]]\n"
     ]
    }
   ],
   "source": [
    "x2 = np.array([[1, 2, 1, 6],  # sample 1\n",
    "               [2, 4, 2, 6],  # sample 2\n",
    "               [1, 2, 1, 6]]) # sample 1 again(!)\n",
    "print(softmax(x2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.8 Softmax derivative if i = j\n",
    "\\begin{align}\n",
    "{g(a)_i} & = \\frac{\\exp({a_i})}{\\sum_{j=1}^n\\exp({a_j})} \\\\\n",
    "Using\\space quotient\\space rule \\space and\\space simplifying... \\\\\n",
    "\\frac{\\partial{g(a)_i}}{\\partial{a_j}}& = \\frac{\\exp({a_i})\\sum_{k=1}^n(\\exp({a_k})-\\exp({a_j}))}\n",
    "{\\sum_{k=1}^n\\exp({a_k})} \\\\\n",
    "& = \\frac{\\exp({a_j})}{\\sum_{k=1}^n\\exp({a_k})} x \n",
    "\\frac{\\sum_{k=1}^n(\\exp({a_k}) - \\exp({a_j}))}{\\sum_{k=1}^n\\exp({a_k})} \\\\\n",
    "& = {p_i}(1 - {p_j})\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.9 Softmax derivative if i != j"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "\\frac{\\partial{g(a)_i}}{\\partial{a_j}}& = \\frac{0 - \\exp({a_i})\\exp({a_j})}\n",
    "{(\\sum_{k=1}^n\\exp({a_k}))^2} \\\\\n",
    "& = \\frac{- \\exp({a_j})}{\\sum_{k=1}^n\\exp({a_k})} x \\frac{\\exp({a_i})}{\\sum_{k=1}^n\\exp({a_k})} \\\\\n",
    "& = -{p_j}{p_i}\n",
    "\\end{align}\n",
    "### 3.10 softmaxp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmaxp(a, e):\n",
    "    ga = softmax(a)\n",
    "    prod = ga * e\n",
    "    dot = (ga * e).sum(axis=0)\n",
    "    return prod - dot.T * ga"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.11 Checking spftmaxp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5.067802993297125e-07, ' should be smaller than 1e-6')\n"
     ]
    }
   ],
   "source": [
    "eps = 1e-6 #finite difference step\n",
    "a = np.random.randn(10, 200) #random inputs\n",
    "e = np.random.randn(10, 200) # random directions\n",
    "diff = softmaxp(a, e)\n",
    "a_soft = softmax(a)\n",
    "diff_approx = (softmax(a + e * eps) - a_soft) / eps\n",
    "rel_error = np.abs(diff - diff_approx).mean() / np.abs(diff_approx).mean()\n",
    "print(rel_error, ' should be smaller than 1e-6')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.12 ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(a):\n",
    "    return a * (a > 0) # for speed\n",
    "def relup(a, e):\n",
    "    return (1 * (a > 0)) * e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Backpropagation\n",
    "### 4.13 Network init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_shallow(Ni, Nh, No):\n",
    "    b1 = np.random.randn(Nh, 1) / np.sqrt((Ni+1.)/2.) \n",
    "    W1 = np.random.randn(Nh, Ni) / np.sqrt((Ni+1.)/2.)\n",
    "    b2 = np.random.randn(No, 1) / np.sqrt((Nh+1.)) \n",
    "    W2 = np.random.randn(No, Nh) / np.sqrt((Nh+1.)) \n",
    "    return W1, b1, W2, b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ni = xtrain.shape[0]\n",
    "Nh = 64\n",
    "No = dtrain.shape[0]\n",
    "netinit = init_shallow(Ni, Nh, No)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.14  Forward prop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forwardprop_shallow(x, net):\n",
    "    W1 = net[0]\n",
    "    b1 = net[1]\n",
    "    W2 = net[2]\n",
    "    b2 = net[3]\n",
    "    \n",
    "    a1 = W1.dot(x) + b1\n",
    "    h1 = relu(a1)\n",
    "    a2 = W2.dot(h1) + b2\n",
    "    y = softmax(a2)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 60000)\n"
     ]
    }
   ],
   "source": [
    "yinit = forwardprop_shallow(xtrain, netinit)\n",
    "print(yinit.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.15 eval loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_loss(y, d):\n",
    "    result = -(d * np.log(y)).sum()/y.size\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.323647544175474, 'should be around 0.26')\n"
     ]
    }
   ],
   "source": [
    "print(eval_loss(yinit, dtrain), 'should be around 0.26')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.16 Eval perfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.942716666667\n"
     ]
    }
   ],
   "source": [
    "def eval_perfs(y, lbl):\n",
    "    y[y.argmax(axis=0), np.arange(0, y.shape[1])] = 1\n",
    "    y[y < 1] = 0\n",
    "    predicted_labels = onehot2label(y)\n",
    "    return 1 - float(np.sum(predicted_labels == lbl))/float(y.shape[1]) \n",
    "print(eval_perfs(yinit, ltrain))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initially the shallow net shows very poor performance.\n",
    "### 4.17 Update shallow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_shallow(x, d, net, gamma=0.05):\n",
    "    W1 = net[0]\n",
    "    b1 = net[1]\n",
    "    W2 = net[2]\n",
    "    b2 = net[3]\n",
    "    Ni = W1.shape[1]\n",
    "    Nh = W1.shape[0]\n",
    "    No = W2.shape[0]\n",
    "    \n",
    "    gamma = gamma / x.shape[1] # normalized by the training dataset size\n",
    "    \n",
    "    # forward phase\n",
    "    a1 = W1.dot(x) + b1 # [10x60k]\n",
    "    h1 = relu(a1)\n",
    "    a2 = W2.dot(h1) + b2 # [64x60k]\n",
    "    y = softmax(a2)\n",
    "    \n",
    "    # error\n",
    "    e = eval_loss(y, d)\n",
    "\n",
    "    # backward phase\n",
    "    delta2 = softmaxp(a2, -d/y)\n",
    "    delta1 = relup(a1, W2.T.dot(delta2))\n",
    "    \n",
    "    # gradient update\n",
    "    W2 = W2 - gamma * delta2.dot(h1.T)\n",
    "    W1 = W1 - gamma * delta1.dot(x.T)\n",
    "    b2 = b2 - gamma * delta2.sum(axis=1).reshape(No, 1)\n",
    "    b1 = b1 - gamma * delta1.sum(axis=1).reshape(Nh, 1)\n",
    "    return W1, b1, W2, b2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.18 Backprop shallow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backprop_shallow(x, d, net, T, gamma=.05): \n",
    "    lbl = onehot2label(d)\n",
    "    for t in range(0, T):\n",
    "        net = update_shallow(x, d, net, gamma)\n",
    "        y = forwardprop_shallow(x, net)\n",
    "        if t % 5 == 0:\n",
    "            print('T: ', t, 'Loss: ', eval_loss(y, d), 'Error: ', eval_perfs(y, lbl))\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('T: ', 0, 'Loss: ', 0.29002366935074414, 'Error: ', 0.8750166666666667)\n",
      "('T: ', 5, 'Loss: ', 0.1988206502104958, 'Error: ', 0.6394666666666666)\n",
      "('T: ', 10, 'Loss: ', 0.16987073971559136, 'Error: ', 0.47865)\n",
      "('T: ', 15, 'Loss: ', 0.14526569695436894, 'Error: ', 0.3618166666666667)\n",
      "('T: ', 20, 'Loss: ', 0.12752412350960265, 'Error: ', 0.3406166666666667)\n",
      "('T: ', 25, 'Loss: ', 0.11523718623131943, 'Error: ', 0.28595000000000004)\n",
      "('T: ', 30, 'Loss: ', 0.1000031510533919, 'Error: ', 0.2675666666666666)\n",
      "('T: ', 35, 'Loss: ', 0.08847981215407838, 'Error: ', 0.21120000000000005)\n",
      "('T: ', 40, 'Loss: ', 0.08096666137039, 'Error: ', 0.20951666666666668)\n",
      "('T: ', 45, 'Loss: ', 0.07509162123082153, 'Error: ', 0.18945)\n",
      "('T: ', 50, 'Loss: ', 0.07060304214140696, 'Error: ', 0.18710000000000004)\n",
      "('T: ', 55, 'Loss: ', 0.06682306639050681, 'Error: ', 0.17584999999999995)\n",
      "('T: ', 60, 'Loss: ', 0.06353597363494799, 'Error: ', 0.1698833333333334)\n",
      "('T: ', 65, 'Loss: ', 0.060793506345731066, 'Error: ', 0.16178333333333328)\n",
      "('T: ', 70, 'Loss: ', 0.05834460946819801, 'Error: ', 0.15656666666666663)\n",
      "('T: ', 75, 'Loss: ', 0.056287527365870456, 'Error: ', 0.15036666666666665)\n",
      "('T: ', 80, 'Loss: ', 0.054459426946422564, 'Error: ', 0.1471)\n",
      "('T: ', 85, 'Loss: ', 0.0528888443300493, 'Error: ', 0.14236666666666664)\n",
      "('T: ', 90, 'Loss: ', 0.051495416632845244, 'Error: ', 0.14003333333333334)\n",
      "('T: ', 95, 'Loss: ', 0.05026916862438356, 'Error: ', 0.13675000000000004)\n"
     ]
    }
   ],
   "source": [
    "nettrain = backprop_shallow(xtrain, dtrain, netinit, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With 100 iterations The error reaches 13.1% and the loss is 0.049.\n",
    "### 4.19 Testing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((784, 10000), (10000,))\n"
     ]
    }
   ],
   "source": [
    "xtest, ltest = MNISTtools.load(dataset = \"testing\", path = \"/datasets/MNIST\")\n",
    "print(xtest.shape, ltest.shape)\n",
    "xtest = normalize_MNIST_images(xtest)\n",
    "dtest = label2onehot(ltest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The testing dataset has 10,000 images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('T: ', 0, 'Loss: ', 0.04705133175014649, 'Error: ', 0.1279)\n",
      "('T: ', 5, 'Loss: ', 0.045860174851585556, 'Error: ', 0.12280000000000002)\n",
      "('T: ', 10, 'Loss: ', 0.04481770360363279, 'Error: ', 0.12070000000000003)\n",
      "('T: ', 15, 'Loss: ', 0.04388281268655895, 'Error: ', 0.11839999999999995)\n",
      "('T: ', 20, 'Loss: ', 0.043034072061693386, 'Error: ', 0.1169)\n",
      "('T: ', 25, 'Loss: ', 0.04225833694785958, 'Error: ', 0.11519999999999997)\n",
      "('T: ', 30, 'Loss: ', 0.041544570720223585, 'Error: ', 0.11350000000000005)\n",
      "('T: ', 35, 'Loss: ', 0.04088497909488696, 'Error: ', 0.11270000000000002)\n",
      "('T: ', 40, 'Loss: ', 0.04027213422859759, 'Error: ', 0.11109999999999998)\n",
      "('T: ', 45, 'Loss: ', 0.03970009936296261, 'Error: ', 0.10929999999999995)\n",
      "('T: ', 50, 'Loss: ', 0.03916435602017621, 'Error: ', 0.10840000000000005)\n",
      "('T: ', 55, 'Loss: ', 0.03866141318236335, 'Error: ', 0.10699999999999998)\n",
      "('T: ', 60, 'Loss: ', 0.03818811426349461, 'Error: ', 0.10619999999999996)\n",
      "('T: ', 65, 'Loss: ', 0.037741444165203454, 'Error: ', 0.1048)\n",
      "('T: ', 70, 'Loss: ', 0.03731878410346896, 'Error: ', 0.1038)\n",
      "('T: ', 75, 'Loss: ', 0.03691757223471042, 'Error: ', 0.10260000000000002)\n",
      "('T: ', 80, 'Loss: ', 0.03653559351172908, 'Error: ', 0.10150000000000003)\n",
      "('T: ', 85, 'Loss: ', 0.03617184342115318, 'Error: ', 0.10060000000000002)\n",
      "('T: ', 90, 'Loss: ', 0.03582538804408562, 'Error: ', 0.09940000000000004)\n",
      "('T: ', 95, 'Loss: ', 0.035494398855040736, 'Error: ', 0.09870000000000001)\n"
     ]
    }
   ],
   "source": [
    "nettest = backprop_shallow(xtest, dtest, nettrain, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The trained network had much better performance and ran much faster. The error after 100 iterations was 9% and the loss was 0.034. \n",
    "### 4.20 Minibatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05\n",
      "('T: ', 0, 'Loss: ', 0.032253286103538975, 'Error: ', 0.09975)\n",
      "('T: ', 1, 'Loss: ', 0.025454713795821916, 'Error: ', 0.0799333333333333)\n",
      "('T: ', 2, 'Loss: ', 0.021422224959094894, 'Error: ', 0.06643333333333334)\n",
      "('T: ', 3, 'Loss: ', 0.018568744170508043, 'Error: ', 0.057466666666666666)\n",
      "('T: ', 4, 'Loss: ', 0.016275084244716475, 'Error: ', 0.050066666666666704)\n"
     ]
    }
   ],
   "source": [
    "def backprop_minibatch_shallow(x, d, net, T, B=100, gamma=.05): \n",
    "    N = x.shape[1]\n",
    "    lbl = onehot2label(d)\n",
    "    for t in range(0, T):\n",
    "        for l in range(0, (N+B-1)/B):\n",
    "            idx = np.arange(B*l, min(B*(l+1), N))\n",
    "            net = update_shallow(x[:,idx], d[:,idx], net, gamma)\n",
    "        y = forwardprop_shallow(x, net)\n",
    "        print('T: ', t, 'Loss: ', eval_loss(y, d), 'Error: ', eval_perfs(y, lbl))    \n",
    "    return net\n",
    "netminibatch = backprop_minibatch_shallow(xtrain, dtrain, netinit, 5, B=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.21 Minibatch testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05\n",
      "('T: ', 0, 'Loss: ', 0.014520325445517953, 'Error: ', 0.04479999999999995)\n",
      "('T: ', 1, 'Loss: ', 0.013567646322539293, 'Error: ', 0.04190000000000005)\n",
      "('T: ', 2, 'Loss: ', 0.012754572414239933, 'Error: ', 0.03979999999999995)\n",
      "('T: ', 3, 'Loss: ', 0.012109770167981473, 'Error: ', 0.03700000000000003)\n",
      "('T: ', 4, 'Loss: ', 0.01144972956659139, 'Error: ', 0.03590000000000004)\n"
     ]
    }
   ],
   "source": [
    "netminibatchtest = backprop_minibatch_shallow(xtest, dtest, netminibatch, 5, B=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The minibatch algorithm is more efficient and has less error. For 5 epochs with minibatch size of 100, the error was 3.5% and the loss was 0.011. This is superior to the standard backprop algorithm.\n",
    "### 5.22 More hidden nodes, minibatch algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('T: ', 0, 'Loss: ', 0.04002498231983609, 'Error: ', 0.12791666666666668)\n",
      "('T: ', 1, 'Loss: ', 0.032066780781495684, 'Error: ', 0.1001333333333333)\n",
      "('T: ', 2, 'Loss: ', 0.02929111365247636, 'Error: ', 0.09145000000000003)\n",
      "('T: ', 3, 'Loss: ', 0.02692495868298967, 'Error: ', 0.0837)\n",
      "('T: ', 4, 'Loss: ', 0.025539702025705695, 'Error: ', 0.07916666666666672)\n"
     ]
    }
   ],
   "source": [
    "netinit16 = init_shallow(Ni, 16, No)\n",
    "nettrain16  = backprop_minibatch_shallow(xtrain, dtrain, netinit16, 5, B=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('T: ', 0, 'Loss: ', 0.024544890735134224, 'Error: ', 0.07563333333333333)\n",
      "('T: ', 1, 'Loss: ', 0.02375407976651502, 'Error: ', 0.07316666666666671)\n",
      "('T: ', 2, 'Loss: ', 0.023269078628798023, 'Error: ', 0.07153333333333334)\n",
      "('T: ', 3, 'Loss: ', 0.022662641515778563, 'Error: ', 0.06963333333333332)\n",
      "('T: ', 4, 'Loss: ', 0.02258204548738588, 'Error: ', 0.06993333333333329)\n"
     ]
    }
   ],
   "source": [
    "nettest16  = backprop_minibatch_shallow(xtrain, dtrain, nettrain16, 5, B=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Increasing the number of hidden units to 16 did not improve the loss or error. At 100 iterations the error was 6.9% and the loss was 0.012."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('T: ', 0, 'Loss: ', 0.030651057480846993, 'Error: ', 0.09550000000000003)\n",
      "('T: ', 1, 'Loss: ', 0.023019820728543514, 'Error: ', 0.0713166666666667)\n",
      "('T: ', 2, 'Loss: ', 0.018792362223676667, 'Error: ', 0.05783333333333329)\n",
      "('T: ', 3, 'Loss: ', 0.015856981211198815, 'Error: ', 0.04795000000000005)\n",
      "('T: ', 4, 'Loss: ', 0.013675140828283574, 'Error: ', 0.04079999999999995)\n"
     ]
    }
   ],
   "source": [
    "netinit256 = init_shallow(Ni, 256, No)\n",
    "nettrain256  = backprop_minibatch_shallow(xtrain, dtrain, netinit256, 5, B=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('T: ', 0, 'Loss: ', 0.012012696133176763, 'Error: ', 0.03598333333333337)\n",
      "('T: ', 1, 'Loss: ', 0.010663653351011835, 'Error: ', 0.03211666666666668)\n",
      "('T: ', 2, 'Loss: ', 0.009620223740400639, 'Error: ', 0.028766666666666718)\n",
      "('T: ', 3, 'Loss: ', 0.008748225762343504, 'Error: ', 0.026116666666666677)\n",
      "('T: ', 4, 'Loss: ', 0.007944758098283487, 'Error: ', 0.02371666666666672)\n"
     ]
    }
   ],
   "source": [
    "nettest256  = backprop_minibatch_shallow(xtrain, dtrain, nettrain256, 5, B=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Increasing the number of hidden units to 256 improved the loss and error over both the 16 hidden unit and 64 hiddden unit models. This leads me to believe that both of these models were underfitting the data. At 100 iterations the error was 2.3% and the loss was 0.007. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.23 Different step sizes\n",
    "gamma = 0.02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.02\n",
      "('T: ', 0, 'Loss: ', 0.038928293379796884, 'Error: ', 0.11911666666666665)\n",
      "('T: ', 1, 'Loss: ', 0.03198821916913864, 'Error: ', 0.09748333333333337)\n",
      "('T: ', 2, 'Loss: ', 0.02831296096315423, 'Error: ', 0.08596666666666664)\n",
      "('T: ', 3, 'Loss: ', 0.025660963952691657, 'Error: ', 0.07773333333333332)\n",
      "('T: ', 4, 'Loss: ', 0.02366880987976596, 'Error: ', 0.07141666666666668)\n"
     ]
    }
   ],
   "source": [
    "nettrain_02 = backprop_minibatch_shallow(xtrain, dtrain, netinit, 5, B=100, gamma=0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.02\n",
      "('T: ', 0, 'Loss: ', 0.021843627724216816, 'Error: ', 0.06559999999999999)\n",
      "('T: ', 1, 'Loss: ', 0.02031203396376363, 'Error: ', 0.06063333333333332)\n",
      "('T: ', 2, 'Loss: ', 0.01901501886472041, 'Error: ', 0.056849999999999956)\n",
      "('T: ', 3, 'Loss: ', 0.017920030612722657, 'Error: ', 0.05386666666666662)\n",
      "('T: ', 4, 'Loss: ', 0.016880657492679536, 'Error: ', 0.0504)\n"
     ]
    }
   ],
   "source": [
    "nettest_02  = backprop_minibatch_shallow(xtrain, dtrain, nettrain_02, 5, B=100, gamma = 0.02)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decreasing gamma to 0.02 from 0.05 leads to a larger error on the test dataset. This may be due to overfitting of the data (stuck in local minimum). <br>\n",
    "gamma = 0.08"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.08\n",
      "('T: ', 0, 'Loss: ', 0.031116383235996945, 'Error: ', 0.0994666666666667)\n",
      "('T: ', 1, 'Loss: ', 0.022813333622077256, 'Error: ', 0.07179999999999997)\n",
      "('T: ', 2, 'Loss: ', 0.018496814994522905, 'Error: ', 0.059116666666666706)\n",
      "('T: ', 3, 'Loss: ', 0.01598138356733656, 'Error: ', 0.05066666666666664)\n",
      "('T: ', 4, 'Loss: ', 0.014396118684122662, 'Error: ', 0.045950000000000046)\n"
     ]
    }
   ],
   "source": [
    "nettrain_08 = backprop_minibatch_shallow(xtrain, dtrain, netinit, 5, B=100, gamma=0.08)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.08\n",
      "('T: ', 0, 'Loss: ', 0.013083715621820858, 'Error: ', 0.041616666666666635)\n",
      "('T: ', 1, 'Loss: ', 0.012171453260641747, 'Error: ', 0.039416666666666655)\n",
      "('T: ', 2, 'Loss: ', 0.011274903915371568, 'Error: ', 0.03634999999999999)\n",
      "('T: ', 3, 'Loss: ', 0.010355135453320634, 'Error: ', 0.03354999999999997)\n",
      "('T: ', 4, 'Loss: ', 0.009528260391666493, 'Error: ', 0.03069999999999995)\n"
     ]
    }
   ],
   "source": [
    "nettest_08  = backprop_minibatch_shallow(xtrain, dtrain, nettrain_08, 5, B=100, gamma = 0.08)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Increasing the step size, gamma, from 0.05 to 0.08 leads to a error of 0.03 and a loss of 0.009 with the test dataset. The training set has loss of 0.014 and error of 0.04. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.24 Different batch sizes\n",
    "B = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05\n",
      "('T: ', 0, 'Loss: ', 0.02633720190635944, 'Error: ', 0.08240000000000003)\n",
      "('T: ', 1, 'Loss: ', 0.020634214637392203, 'Error: ', 0.0662166666666667)\n",
      "('T: ', 2, 'Loss: ', 0.017636861751038357, 'Error: ', 0.05689999999999995)\n",
      "('T: ', 3, 'Loss: ', 0.015528802495383546, 'Error: ', 0.05015000000000003)\n",
      "('T: ', 4, 'Loss: ', 0.013797515601065595, 'Error: ', 0.044849999999999945)\n"
     ]
    }
   ],
   "source": [
    "nettrain50 = backprop_minibatch_shallow(xtrain, dtrain, netinit, 5, B=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05\n",
      "('T: ', 0, 'Loss: ', 0.013097869407156472, 'Error: ', 0.04235)\n",
      "('T: ', 1, 'Loss: ', 0.011921564396522252, 'Error: ', 0.038516666666666644)\n",
      "('T: ', 2, 'Loss: ', 0.011379999108697043, 'Error: ', 0.0369666666666667)\n",
      "('T: ', 3, 'Loss: ', 0.01121387397381039, 'Error: ', 0.036800000000000055)\n",
      "('T: ', 4, 'Loss: ', 0.01045096224582522, 'Error: ', 0.03420000000000001)\n"
     ]
    }
   ],
   "source": [
    "nettest50 = backprop_minibatch_shallow(xtrain, dtrain, nettrain50, 5, B=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "B = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05\n",
      "('T: ', 0, 'Loss: ', 0.03696149491159031, 'Error: ', 0.11380000000000001)\n",
      "('T: ', 1, 'Loss: ', 0.030014186415876637, 'Error: ', 0.09189999999999998)\n",
      "('T: ', 2, 'Loss: ', 0.026220630489559162, 'Error: ', 0.07999999999999996)\n",
      "('T: ', 3, 'Loss: ', 0.023440572460521936, 'Error: ', 0.07089999999999996)\n",
      "('T: ', 4, 'Loss: ', 0.021185398515450767, 'Error: ', 0.06398333333333328)\n"
     ]
    }
   ],
   "source": [
    "nettrain200 = backprop_minibatch_shallow(xtrain, dtrain, netinit, 5, B=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05\n",
      "('T: ', 0, 'Loss: ', 0.019344751217925046, 'Error: ', 0.05845)\n",
      "('T: ', 1, 'Loss: ', 0.017882536262317415, 'Error: ', 0.05413333333333337)\n",
      "('T: ', 2, 'Loss: ', 0.01659804487985277, 'Error: ', 0.05010000000000003)\n",
      "('T: ', 3, 'Loss: ', 0.015528618299999443, 'Error: ', 0.046666666666666634)\n",
      "('T: ', 4, 'Loss: ', 0.014613277709524418, 'Error: ', 0.04353333333333331)\n"
     ]
    }
   ],
   "source": [
    "nettest200 = backprop_minibatch_shallow(xtrain, dtrain, nettrain200, 5, B=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Increaseing the batch size to 200 caused a slight increase in the test dataset error. This is indicative of overfitting. For the test dataset the error is 0.04. For the training set the error is 0.06"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.25 Increasing the number of epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05\n",
      "('T: ', 0, 'Loss: ', 0.032253286103538975, 'Error: ', 0.09975)\n",
      "('T: ', 1, 'Loss: ', 0.025454713795821916, 'Error: ', 0.0799333333333333)\n",
      "('T: ', 2, 'Loss: ', 0.021422224959094894, 'Error: ', 0.06643333333333334)\n",
      "('T: ', 3, 'Loss: ', 0.018568744170508043, 'Error: ', 0.057466666666666666)\n",
      "('T: ', 4, 'Loss: ', 0.016275084244716475, 'Error: ', 0.050066666666666704)\n",
      "('T: ', 5, 'Loss: ', 0.01467513678751892, 'Error: ', 0.044950000000000045)\n",
      "('T: ', 6, 'Loss: ', 0.013186028447973104, 'Error: ', 0.040133333333333354)\n",
      "('T: ', 7, 'Loss: ', 0.012067279779174624, 'Error: ', 0.03651666666666664)\n",
      "('T: ', 8, 'Loss: ', 0.011184302836167812, 'Error: ', 0.03363333333333329)\n",
      "('T: ', 9, 'Loss: ', 0.010382486224947645, 'Error: ', 0.031100000000000017)\n",
      "('T: ', 10, 'Loss: ', 0.009727160428358759, 'Error: ', 0.029249999999999998)\n",
      "('T: ', 11, 'Loss: ', 0.009148478434711121, 'Error: ', 0.027766666666666717)\n",
      "('T: ', 12, 'Loss: ', 0.008720399140228135, 'Error: ', 0.02643333333333331)\n",
      "('T: ', 13, 'Loss: ', 0.00831273595595609, 'Error: ', 0.02529999999999999)\n",
      "('T: ', 14, 'Loss: ', 0.007955091186856418, 'Error: ', 0.024533333333333296)\n",
      "('T: ', 15, 'Loss: ', 0.007648692750032943, 'Error: ', 0.023566666666666625)\n",
      "('T: ', 16, 'Loss: ', 0.007326213081947873, 'Error: ', 0.022399999999999975)\n",
      "('T: ', 17, 'Loss: ', 0.007029904303158341, 'Error: ', 0.021583333333333288)\n",
      "('T: ', 18, 'Loss: ', 0.0068145802428096446, 'Error: ', 0.020883333333333365)\n",
      "('T: ', 19, 'Loss: ', 0.006545221431111654, 'Error: ', 0.02018333333333333)\n",
      "('T: ', 20, 'Loss: ', 0.006325216027374533, 'Error: ', 0.019483333333333297)\n",
      "('T: ', 21, 'Loss: ', 0.006077173878983563, 'Error: ', 0.01856666666666662)\n",
      "('T: ', 22, 'Loss: ', 0.005904719033125843, 'Error: ', 0.018333333333333313)\n",
      "('T: ', 23, 'Loss: ', 0.005689811077341755, 'Error: ', 0.017683333333333384)\n",
      "('T: ', 24, 'Loss: ', 0.0055707881110552615, 'Error: ', 0.01739999999999997)\n",
      "('T: ', 25, 'Loss: ', 0.005361634724729215, 'Error: ', 0.01663333333333339)\n",
      "('T: ', 26, 'Loss: ', 0.0052270408930108845, 'Error: ', 0.016199999999999992)\n",
      "('T: ', 27, 'Loss: ', 0.005077541497406492, 'Error: ', 0.015833333333333366)\n",
      "('T: ', 28, 'Loss: ', 0.0048902607441214845, 'Error: ', 0.015349999999999975)\n",
      "('T: ', 29, 'Loss: ', 0.004796777016519307, 'Error: ', 0.015066666666666673)\n",
      "('T: ', 30, 'Loss: ', 0.004690491136256707, 'Error: ', 0.014633333333333387)\n",
      "('T: ', 31, 'Loss: ', 0.004585561284921874, 'Error: ', 0.014549999999999952)\n",
      "('T: ', 32, 'Loss: ', 0.004468222220774554, 'Error: ', 0.0138166666666667)\n",
      "('T: ', 33, 'Loss: ', 0.004382227178962781, 'Error: ', 0.01378333333333337)\n",
      "('T: ', 34, 'Loss: ', 0.0042960310275858395, 'Error: ', 0.013433333333333297)\n",
      "('T: ', 35, 'Loss: ', 0.004148811207254345, 'Error: ', 0.013000000000000012)\n",
      "('T: ', 36, 'Loss: ', 0.0040800554740946255, 'Error: ', 0.012666666666666715)\n",
      "('T: ', 37, 'Loss: ', 0.004009655382088021, 'Error: ', 0.012700000000000045)\n",
      "('T: ', 38, 'Loss: ', 0.003888743933952213, 'Error: ', 0.012333333333333307)\n",
      "('T: ', 39, 'Loss: ', 0.0038108869146625717, 'Error: ', 0.01206666666666667)\n",
      "('T: ', 40, 'Loss: ', 0.0036897336438877484, 'Error: ', 0.011583333333333279)\n",
      "('T: ', 41, 'Loss: ', 0.003697575724651525, 'Error: ', 0.011633333333333384)\n",
      "('T: ', 42, 'Loss: ', 0.003678942601577896, 'Error: ', 0.011800000000000033)\n",
      "('T: ', 43, 'Loss: ', 0.0034901729990628036, 'Error: ', 0.01100000000000001)\n",
      "('T: ', 44, 'Loss: ', 0.003405339338295283, 'Error: ', 0.010783333333333367)\n",
      "('T: ', 45, 'Loss: ', 0.003365218362090255, 'Error: ', 0.010633333333333383)\n",
      "('T: ', 46, 'Loss: ', 0.003357060964717682, 'Error: ', 0.010800000000000032)\n",
      "('T: ', 47, 'Loss: ', 0.003280021636569312, 'Error: ', 0.010433333333333294)\n",
      "('T: ', 48, 'Loss: ', 0.003230300073508294, 'Error: ', 0.010166666666666657)\n",
      "('T: ', 49, 'Loss: ', 0.0031321875153426004, 'Error: ', 0.010149999999999992)\n",
      "('T: ', 50, 'Loss: ', 0.0031128799699252775, 'Error: ', 0.009850000000000025)\n",
      "('T: ', 51, 'Loss: ', 0.0030189865005515962, 'Error: ', 0.009666666666666712)\n",
      "('T: ', 52, 'Loss: ', 0.0029376723189498163, 'Error: ', 0.009399999999999964)\n",
      "('T: ', 53, 'Loss: ', 0.0029114195821505996, 'Error: ', 0.009383333333333299)\n",
      "('T: ', 54, 'Loss: ', 0.0028729758128493133, 'Error: ', 0.009233333333333316)\n",
      "('T: ', 55, 'Loss: ', 0.0027432994389782893, 'Error: ', 0.00880000000000003)\n",
      "('T: ', 56, 'Loss: ', 0.0027130209695538482, 'Error: ', 0.00883333333333336)\n",
      "('T: ', 57, 'Loss: ', 0.002676412196775267, 'Error: ', 0.008683333333333376)\n",
      "('T: ', 58, 'Loss: ', 0.0025869324978857735, 'Error: ', 0.008449999999999958)\n",
      "('T: ', 59, 'Loss: ', 0.002527773620566693, 'Error: ', 0.008133333333333326)\n",
      "('T: ', 60, 'Loss: ', 0.0025054573305057217, 'Error: ', 0.008000000000000007)\n",
      "('T: ', 61, 'Loss: ', 0.0024674205102214425, 'Error: ', 0.007950000000000013)\n",
      "('T: ', 62, 'Loss: ', 0.002423389124375562, 'Error: ', 0.007816666666666694)\n",
      "('T: ', 63, 'Loss: ', 0.002402404916634362, 'Error: ', 0.007633333333333381)\n",
      "('T: ', 64, 'Loss: ', 0.002356848478414866, 'Error: ', 0.007633333333333381)\n",
      "('T: ', 65, 'Loss: ', 0.002328375414268982, 'Error: ', 0.007516666666666616)\n",
      "('T: ', 66, 'Loss: ', 0.002239817176249522, 'Error: ', 0.00714999999999999)\n",
      "('T: ', 67, 'Loss: ', 0.0021985984789941134, 'Error: ', 0.007099999999999995)\n",
      "('T: ', 68, 'Loss: ', 0.0021839699258271103, 'Error: ', 0.007033333333333336)\n",
      "('T: ', 69, 'Loss: ', 0.002135342225314214, 'Error: ', 0.006833333333333358)\n",
      "('T: ', 70, 'Loss: ', 0.0020791936352954304, 'Error: ', 0.006683333333333374)\n",
      "('T: ', 71, 'Loss: ', 0.002097805221958141, 'Error: ', 0.006866666666666688)\n",
      "('T: ', 72, 'Loss: ', 0.0020148094349335044, 'Error: ', 0.006516666666666615)\n",
      "('T: ', 73, 'Loss: ', 0.0020238563231948672, 'Error: ', 0.0065833333333333854)\n",
      "('T: ', 74, 'Loss: ', 0.0019463867815092094, 'Error: ', 0.0063833333333332964)\n",
      "('T: ', 75, 'Loss: ', 0.0019226815991690873, 'Error: ', 0.006066666666666665)\n",
      "('T: ', 76, 'Loss: ', 0.001920573177034017, 'Error: ', 0.006133333333333324)\n",
      "('T: ', 77, 'Loss: ', 0.001852461948288175, 'Error: ', 0.005966666666666676)\n",
      "('T: ', 78, 'Loss: ', 0.001799226583574725, 'Error: ', 0.005633333333333379)\n",
      "('T: ', 79, 'Loss: ', 0.0017797081111299796, 'Error: ', 0.005650000000000044)\n",
      "('T: ', 80, 'Loss: ', 0.0017355087036413014, 'Error: ', 0.0053833333333332956)\n",
      "('T: ', 81, 'Loss: ', 0.0017011866380759366, 'Error: ', 0.005099999999999993)\n",
      "('T: ', 82, 'Loss: ', 0.0016941257043368682, 'Error: ', 0.005149999999999988)\n",
      "('T: ', 83, 'Loss: ', 0.00165368099273181, 'Error: ', 0.005033333333333334)\n",
      "('T: ', 84, 'Loss: ', 0.0016614805758166063, 'Error: ', 0.005166666666666653)\n",
      "('T: ', 85, 'Loss: ', 0.0016017262191279293, 'Error: ', 0.004833333333333356)\n",
      "('T: ', 86, 'Loss: ', 0.001559661520363244, 'Error: ', 0.004683333333333373)\n",
      "('T: ', 87, 'Loss: ', 0.0015549317970011058, 'Error: ', 0.0047000000000000375)\n",
      "('T: ', 88, 'Loss: ', 0.001492758215387014, 'Error: ', 0.004483333333333284)\n",
      "('T: ', 89, 'Loss: ', 0.0014988487851329375, 'Error: ', 0.004466666666666619)\n",
      "('T: ', 90, 'Loss: ', 0.0014671597456882039, 'Error: ', 0.004383333333333295)\n",
      "('T: ', 91, 'Loss: ', 0.0014211997024194526, 'Error: ', 0.004166666666666652)\n",
      "('T: ', 92, 'Loss: ', 0.0014375245185397011, 'Error: ', 0.004316666666666635)\n",
      "('T: ', 93, 'Loss: ', 0.0013975161125274755, 'Error: ', 0.004183333333333317)\n",
      "('T: ', 94, 'Loss: ', 0.0013925311202918466, 'Error: ', 0.004183333333333317)\n",
      "('T: ', 95, 'Loss: ', 0.0013407975204119597, 'Error: ', 0.0038166666666666904)\n",
      "('T: ', 96, 'Loss: ', 0.0013219799078797345, 'Error: ', 0.0038833333333333497)\n",
      "('T: ', 97, 'Loss: ', 0.0013167194102018803, 'Error: ', 0.0038000000000000256)\n",
      "('T: ', 98, 'Loss: ', 0.0012641724130915888, 'Error: ', 0.003650000000000042)\n",
      "('T: ', 99, 'Loss: ', 0.0012720442023310028, 'Error: ', 0.0037166666666667014)\n",
      "('T: ', 100, 'Loss: ', 0.0012066667368647631, 'Error: ', 0.003349999999999964)\n",
      "('T: ', 101, 'Loss: ', 0.0011807650555290675, 'Error: ', 0.00326666666666664)\n",
      "('T: ', 102, 'Loss: ', 0.0011769460954743766, 'Error: ', 0.0032999999999999696)\n",
      "('T: ', 103, 'Loss: ', 0.0011235127985122103, 'Error: ', 0.003049999999999997)\n",
      "('T: ', 104, 'Loss: ', 0.0011044090152368012, 'Error: ', 0.0029166666666666785)\n",
      "('T: ', 105, 'Loss: ', 0.0010637982510233424, 'Error: ', 0.002866666666666684)\n",
      "('T: ', 106, 'Loss: ', 0.0010395321315381939, 'Error: ', 0.0027000000000000357)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('T: ', 107, 'Loss: ', 0.0009982454893644815, 'Error: ', 0.0026000000000000467)\n",
      "('T: ', 108, 'Loss: ', 0.0009865465810475291, 'Error: ', 0.0025500000000000522)\n",
      "('T: ', 109, 'Loss: ', 0.0009662633200858451, 'Error: ', 0.002466666666666617)\n",
      "('T: ', 110, 'Loss: ', 0.0009503885708668903, 'Error: ', 0.002466666666666617)\n",
      "('T: ', 111, 'Loss: ', 0.0009261526381955648, 'Error: ', 0.0023333333333332984)\n",
      "('T: ', 112, 'Loss: ', 0.0008714640168470119, 'Error: ', 0.001983333333333337)\n",
      "('T: ', 113, 'Loss: ', 0.000889642089170786, 'Error: ', 0.002083333333333326)\n",
      "('T: ', 114, 'Loss: ', 0.0008618370122423979, 'Error: ', 0.0019166666666666776)\n",
      "('T: ', 115, 'Loss: ', 0.0008458182429519014, 'Error: ', 0.0019000000000000128)\n",
      "('T: ', 116, 'Loss: ', 0.0008061843998343175, 'Error: ', 0.0017500000000000293)\n",
      "('T: ', 117, 'Loss: ', 0.000823388960352795, 'Error: ', 0.0018666666666666831)\n",
      "('T: ', 118, 'Loss: ', 0.0008049586333840986, 'Error: ', 0.0017500000000000293)\n",
      "('T: ', 119, 'Loss: ', 0.0007741417208654562, 'Error: ', 0.0016333333333333755)\n",
      "('T: ', 120, 'Loss: ', 0.0007736329545402392, 'Error: ', 0.00168333333333337)\n",
      "('T: ', 121, 'Loss: ', 0.0007385561199041179, 'Error: ', 0.0015166666666667217)\n",
      "('T: ', 122, 'Loss: ', 0.0007390297575311287, 'Error: ', 0.0016000000000000458)\n",
      "('T: ', 123, 'Loss: ', 0.0007284989638758003, 'Error: ', 0.001483333333333281)\n",
      "('T: ', 124, 'Loss: ', 0.0007175433246300653, 'Error: ', 0.001383333333333292)\n",
      "('T: ', 125, 'Loss: ', 0.0006903376479287814, 'Error: ', 0.0013333333333332975)\n",
      "('T: ', 126, 'Loss: ', 0.000688857516222408, 'Error: ', 0.0013333333333332975)\n",
      "('T: ', 127, 'Loss: ', 0.0006608679086833103, 'Error: ', 0.0012999999999999678)\n",
      "('T: ', 128, 'Loss: ', 0.0006751963124561258, 'Error: ', 0.0013333333333332975)\n",
      "('T: ', 129, 'Loss: ', 0.0006614244455176531, 'Error: ', 0.0012666666666666382)\n",
      "('T: ', 130, 'Loss: ', 0.0006477030027090204, 'Error: ', 0.0011999999999999789)\n",
      "('T: ', 131, 'Loss: ', 0.0006320592431705623, 'Error: ', 0.0011166666666666547)\n",
      "('T: ', 132, 'Loss: ', 0.0006156471277274532, 'Error: ', 0.001083333333333325)\n",
      "('T: ', 133, 'Loss: ', 0.0006194627001678464, 'Error: ', 0.0011166666666666547)\n",
      "('T: ', 134, 'Loss: ', 0.0005981651437546626, 'Error: ', 0.000983333333333336)\n",
      "('T: ', 135, 'Loss: ', 0.0005990536828978047, 'Error: ', 0.0009666666666666712)\n",
      "('T: ', 136, 'Loss: ', 0.0005861119577387883, 'Error: ', 0.000983333333333336)\n",
      "('T: ', 137, 'Loss: ', 0.0005659009304369874, 'Error: ', 0.0008500000000000174)\n",
      "('T: ', 138, 'Loss: ', 0.000574006134431843, 'Error: ', 0.0009666666666666712)\n",
      "('T: ', 139, 'Loss: ', 0.0005593165262173687, 'Error: ', 0.0008166666666666877)\n",
      "('T: ', 140, 'Loss: ', 0.0005531288140908715, 'Error: ', 0.0008333333333333526)\n",
      "('T: ', 141, 'Loss: ', 0.000540447296520468, 'Error: ', 0.0007833333333333581)\n",
      "('T: ', 142, 'Loss: ', 0.000531297528401268, 'Error: ', 0.0007500000000000284)\n",
      "('T: ', 143, 'Loss: ', 0.0005290140003904361, 'Error: ', 0.0006833333333333691)\n",
      "('T: ', 144, 'Loss: ', 0.0005252230570850032, 'Error: ', 0.0007500000000000284)\n",
      "('T: ', 145, 'Loss: ', 0.0005107920835687859, 'Error: ', 0.0006666666666667043)\n",
      "('T: ', 146, 'Loss: ', 0.000504428636798101, 'Error: ', 0.0006166666666667098)\n",
      "('T: ', 147, 'Loss: ', 0.0004940388517939125, 'Error: ', 0.0006000000000000449)\n",
      "('T: ', 148, 'Loss: ', 0.00048796948458606484, 'Error: ', 0.0006333333333333746)\n",
      "('T: ', 149, 'Loss: ', 0.00048296908594597926, 'Error: ', 0.0005833333333333801)\n",
      "('T: ', 150, 'Loss: ', 0.00047757908519510634, 'Error: ', 0.0005833333333333801)\n",
      "('T: ', 151, 'Loss: ', 0.0004673785811836118, 'Error: ', 0.0006000000000000449)\n",
      "('T: ', 152, 'Loss: ', 0.00046232414779820744, 'Error: ', 0.0005833333333333801)\n",
      "('T: ', 153, 'Loss: ', 0.0004573888460726306, 'Error: ', 0.0005166666666667208)\n",
      "('T: ', 154, 'Loss: ', 0.0004495194859612393, 'Error: ', 0.0004999999999999449)\n",
      "('T: ', 155, 'Loss: ', 0.0004466800789079563, 'Error: ', 0.0004833333333332801)\n",
      "('T: ', 156, 'Loss: ', 0.0004419395467160075, 'Error: ', 0.0005166666666667208)\n",
      "('T: ', 157, 'Loss: ', 0.00043101541035258246, 'Error: ', 0.0004833333333332801)\n",
      "('T: ', 158, 'Loss: ', 0.0004281869779672787, 'Error: ', 0.00046666666666661527)\n",
      "('T: ', 159, 'Loss: ', 0.00042160622178761707, 'Error: ', 0.0004166666666666208)\n",
      "('T: ', 160, 'Loss: ', 0.0004142918076789261, 'Error: ', 0.0003833333333332911)\n",
      "('T: ', 161, 'Loss: ', 0.00041039116497815827, 'Error: ', 0.00039999999999995595)\n",
      "('T: ', 162, 'Loss: ', 0.0004072790464042061, 'Error: ', 0.0003833333333332911)\n",
      "('T: ', 163, 'Loss: ', 0.0004006366746241449, 'Error: ', 0.0003333333333332966)\n",
      "('T: ', 164, 'Loss: ', 0.0003952998271544904, 'Error: ', 0.00034999999999996145)\n",
      "('T: ', 165, 'Loss: ', 0.00039164978819921314, 'Error: ', 0.0003166666666666318)\n",
      "('T: ', 166, 'Loss: ', 0.00038555928511116417, 'Error: ', 0.0003166666666666318)\n",
      "('T: ', 167, 'Loss: ', 0.00038207707724182034, 'Error: ', 0.0003166666666666318)\n",
      "('T: ', 168, 'Loss: ', 0.0003743207606521527, 'Error: ', 0.0003166666666666318)\n",
      "('T: ', 169, 'Loss: ', 0.00036681115785992097, 'Error: ', 0.00024999999999997247)\n",
      "('T: ', 170, 'Loss: ', 0.0003676032547272369, 'Error: ', 0.0002666666666666373)\n",
      "('T: ', 171, 'Loss: ', 0.0003652906946121074, 'Error: ', 0.00029999999999996696)\n",
      "('T: ', 172, 'Loss: ', 0.00035959445248818497, 'Error: ', 0.0002666666666666373)\n",
      "('T: ', 173, 'Loss: ', 0.0003555430699035488, 'Error: ', 0.0002166666666666428)\n",
      "('T: ', 174, 'Loss: ', 0.0003498650314685427, 'Error: ', 0.00023333333333330764)\n",
      "('T: ', 175, 'Loss: ', 0.0003454757269170755, 'Error: ', 0.0002166666666666428)\n",
      "('T: ', 176, 'Loss: ', 0.00034303533502858284, 'Error: ', 0.00018333333333331314)\n",
      "('T: ', 177, 'Loss: ', 0.0003386917020846986, 'Error: ', 0.00018333333333331314)\n",
      "('T: ', 178, 'Loss: ', 0.00033618549077464116, 'Error: ', 0.0001666666666666483)\n",
      "('T: ', 179, 'Loss: ', 0.00032991371009177925, 'Error: ', 0.00018333333333331314)\n",
      "('T: ', 180, 'Loss: ', 0.00032784321452246633, 'Error: ', 0.00014999999999998348)\n",
      "('T: ', 181, 'Loss: ', 0.0003237983407644082, 'Error: ', 0.0001666666666666483)\n",
      "('T: ', 182, 'Loss: ', 0.00031972493628514795, 'Error: ', 0.00014999999999998348)\n",
      "('T: ', 183, 'Loss: ', 0.0003164976693607319, 'Error: ', 0.00014999999999998348)\n",
      "('T: ', 184, 'Loss: ', 0.00031520570982841137, 'Error: ', 0.00014999999999998348)\n",
      "('T: ', 185, 'Loss: ', 0.0003061996566632108, 'Error: ', 0.00013333333333331865)\n",
      "('T: ', 186, 'Loss: ', 0.00030837622952852836, 'Error: ', 0.00014999999999998348)\n",
      "('T: ', 187, 'Loss: ', 0.00030128068103344667, 'Error: ', 0.00011666666666665382)\n",
      "('T: ', 188, 'Loss: ', 0.0003020678979562056, 'Error: ', 0.00013333333333331865)\n",
      "('T: ', 189, 'Loss: ', 0.00029395086693529375, 'Error: ', 0.00011666666666665382)\n",
      "('T: ', 190, 'Loss: ', 0.0002924486887750467, 'Error: ', 0.00011666666666665382)\n",
      "('T: ', 191, 'Loss: ', 0.0002890829468032346, 'Error: ', 0.00011666666666665382)\n",
      "('T: ', 192, 'Loss: ', 0.0002864204912336334, 'Error: ', 0.00011666666666665382)\n",
      "('T: ', 193, 'Loss: ', 0.0002849451545805642, 'Error: ', 0.00011666666666665382)\n",
      "('T: ', 194, 'Loss: ', 0.0002790772083457896, 'Error: ', 9.999999999998899e-05)\n",
      "('T: ', 195, 'Loss: ', 0.0002781092624264055, 'Error: ', 9.999999999998899e-05)\n",
      "('T: ', 196, 'Loss: ', 0.00027674847141399297, 'Error: ', 9.999999999998899e-05)\n",
      "('T: ', 197, 'Loss: ', 0.00027253079873975326, 'Error: ', 9.999999999998899e-05)\n",
      "('T: ', 198, 'Loss: ', 0.0002680770150519012, 'Error: ', 8.333333333332416e-05)\n",
      "('T: ', 199, 'Loss: ', 0.00026622276626123116, 'Error: ', 8.333333333332416e-05)\n"
     ]
    }
   ],
   "source": [
    "epochtrain = backprop_minibatch_shallow(xtrain, dtrain, netinit, 200, B=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05\n",
      "('T: ', 0, 'Loss: ', 0.00026162431160159887, 'Error: ', 8.333333333332416e-05)\n",
      "('T: ', 1, 'Loss: ', 0.000262739583310177, 'Error: ', 8.333333333332416e-05)\n",
      "('T: ', 2, 'Loss: ', 0.0002567947510857342, 'Error: ', 8.333333333332416e-05)\n",
      "('T: ', 3, 'Loss: ', 0.0002549720117022428, 'Error: ', 8.333333333332416e-05)\n",
      "('T: ', 4, 'Loss: ', 0.00025374907235304304, 'Error: ', 8.333333333332416e-05)\n",
      "('T: ', 5, 'Loss: ', 0.0002492168197916451, 'Error: ', 8.333333333332416e-05)\n",
      "('T: ', 6, 'Loss: ', 0.0002485775386143003, 'Error: ', 8.333333333332416e-05)\n",
      "('T: ', 7, 'Loss: ', 0.00024627712882617904, 'Error: ', 8.333333333332416e-05)\n",
      "('T: ', 8, 'Loss: ', 0.00024124641464205653, 'Error: ', 4.999999999999449e-05)\n",
      "('T: ', 9, 'Loss: ', 0.00024037421254307788, 'Error: ', 6.666666666665932e-05)\n",
      "('T: ', 10, 'Loss: ', 0.00023832300177662868, 'Error: ', 8.333333333332416e-05)\n",
      "('T: ', 11, 'Loss: ', 0.00023538067000013123, 'Error: ', 8.333333333332416e-05)\n",
      "('T: ', 12, 'Loss: ', 0.00023372105973152037, 'Error: ', 8.333333333332416e-05)\n",
      "('T: ', 13, 'Loss: ', 0.00023083326863847166, 'Error: ', 4.999999999999449e-05)\n",
      "('T: ', 14, 'Loss: ', 0.00022891129073909436, 'Error: ', 4.999999999999449e-05)\n",
      "('T: ', 15, 'Loss: ', 0.0002257217451580233, 'Error: ', 6.666666666665932e-05)\n",
      "('T: ', 16, 'Loss: ', 0.00022455089331234598, 'Error: ', 6.666666666665932e-05)\n",
      "('T: ', 17, 'Loss: ', 0.00022362371349058486, 'Error: ', 4.999999999999449e-05)\n",
      "('T: ', 18, 'Loss: ', 0.00022093165682701203, 'Error: ', 4.999999999999449e-05)\n",
      "('T: ', 19, 'Loss: ', 0.0002175705932420275, 'Error: ', 4.999999999999449e-05)\n",
      "('T: ', 20, 'Loss: ', 0.0002166394692568208, 'Error: ', 6.666666666665932e-05)\n",
      "('T: ', 21, 'Loss: ', 0.00021541008500549193, 'Error: ', 4.999999999999449e-05)\n",
      "('T: ', 22, 'Loss: ', 0.00021141953914451833, 'Error: ', 4.999999999999449e-05)\n",
      "('T: ', 23, 'Loss: ', 0.0002117823223010564, 'Error: ', 4.999999999999449e-05)\n",
      "('T: ', 24, 'Loss: ', 0.00020799300340834412, 'Error: ', 4.999999999999449e-05)\n",
      "('T: ', 25, 'Loss: ', 0.00020741330304868688, 'Error: ', 4.999999999999449e-05)\n",
      "('T: ', 26, 'Loss: ', 0.0002052720085514197, 'Error: ', 4.999999999999449e-05)\n",
      "('T: ', 27, 'Loss: ', 0.0002021405675256765, 'Error: ', 4.999999999999449e-05)\n",
      "('T: ', 28, 'Loss: ', 0.0002010085680094436, 'Error: ', 4.999999999999449e-05)\n",
      "('T: ', 29, 'Loss: ', 0.00019937978407245327, 'Error: ', 3.333333333332966e-05)\n",
      "('T: ', 30, 'Loss: ', 0.00019766441850456557, 'Error: ', 3.333333333332966e-05)\n",
      "('T: ', 31, 'Loss: ', 0.00019579275884694643, 'Error: ', 3.333333333332966e-05)\n",
      "('T: ', 32, 'Loss: ', 0.00019426481985974803, 'Error: ', 3.333333333332966e-05)\n",
      "('T: ', 33, 'Loss: ', 0.0001917332329341101, 'Error: ', 3.333333333332966e-05)\n",
      "('T: ', 34, 'Loss: ', 0.00019089958231981817, 'Error: ', 3.333333333332966e-05)\n",
      "('T: ', 35, 'Loss: ', 0.00018751703142392046, 'Error: ', 3.333333333332966e-05)\n",
      "('T: ', 36, 'Loss: ', 0.00018828511424440517, 'Error: ', 3.333333333332966e-05)\n",
      "('T: ', 37, 'Loss: ', 0.00018509814377162522, 'Error: ', 3.333333333332966e-05)\n",
      "('T: ', 38, 'Loss: ', 0.0001836826548786573, 'Error: ', 3.333333333332966e-05)\n",
      "('T: ', 39, 'Loss: ', 0.00018272618810117336, 'Error: ', 3.333333333332966e-05)\n",
      "('T: ', 40, 'Loss: ', 0.00018244351499896463, 'Error: ', 3.333333333332966e-05)\n",
      "('T: ', 41, 'Loss: ', 0.0001799902751716239, 'Error: ', 3.333333333332966e-05)\n",
      "('T: ', 42, 'Loss: ', 0.00017955135075138146, 'Error: ', 3.333333333332966e-05)\n",
      "('T: ', 43, 'Loss: ', 0.0001762412912182501, 'Error: ', 3.333333333332966e-05)\n",
      "('T: ', 44, 'Loss: ', 0.0001760179897899632, 'Error: ', 3.333333333332966e-05)\n",
      "('T: ', 45, 'Loss: ', 0.00017263195175120743, 'Error: ', 1.666666666666483e-05)\n",
      "('T: ', 46, 'Loss: ', 0.00017150179340716005, 'Error: ', 1.666666666666483e-05)\n",
      "('T: ', 47, 'Loss: ', 0.00017112033317744226, 'Error: ', 3.333333333332966e-05)\n",
      "('T: ', 48, 'Loss: ', 0.00017009466835342173, 'Error: ', 1.666666666666483e-05)\n",
      "('T: ', 49, 'Loss: ', 0.00016806432696197388, 'Error: ', 1.666666666666483e-05)\n",
      "('T: ', 50, 'Loss: ', 0.0001675051792111161, 'Error: ', 1.666666666666483e-05)\n",
      "('T: ', 51, 'Loss: ', 0.00016548191502925688, 'Error: ', 1.666666666666483e-05)\n",
      "('T: ', 52, 'Loss: ', 0.0001636271247239661, 'Error: ', 1.666666666666483e-05)\n",
      "('T: ', 53, 'Loss: ', 0.0001632251753283113, 'Error: ', 1.666666666666483e-05)\n",
      "('T: ', 54, 'Loss: ', 0.00016119066358329635, 'Error: ', 1.666666666666483e-05)\n",
      "('T: ', 55, 'Loss: ', 0.00016091925362547146, 'Error: ', 1.666666666666483e-05)\n",
      "('T: ', 56, 'Loss: ', 0.00015957553806228618, 'Error: ', 1.666666666666483e-05)\n",
      "('T: ', 57, 'Loss: ', 0.00015758075852286536, 'Error: ', 1.666666666666483e-05)\n",
      "('T: ', 58, 'Loss: ', 0.00015636691584424982, 'Error: ', 1.666666666666483e-05)\n",
      "('T: ', 59, 'Loss: ', 0.00015473534132236266, 'Error: ', 1.666666666666483e-05)\n",
      "('T: ', 60, 'Loss: ', 0.00015462044859335878, 'Error: ', 1.666666666666483e-05)\n",
      "('T: ', 61, 'Loss: ', 0.00015354846676628108, 'Error: ', 1.666666666666483e-05)\n",
      "('T: ', 62, 'Loss: ', 0.00015076359097933656, 'Error: ', 1.666666666666483e-05)\n",
      "('T: ', 63, 'Loss: ', 0.0001502774048338327, 'Error: ', 1.666666666666483e-05)\n",
      "('T: ', 64, 'Loss: ', 0.00014962227330636444, 'Error: ', 1.666666666666483e-05)\n",
      "('T: ', 65, 'Loss: ', 0.00014785994337660215, 'Error: ', 1.666666666666483e-05)\n",
      "('T: ', 66, 'Loss: ', 0.0001478241410258295, 'Error: ', 1.666666666666483e-05)\n",
      "('T: ', 67, 'Loss: ', 0.00014624420101304341, 'Error: ', 1.666666666666483e-05)\n",
      "('T: ', 68, 'Loss: ', 0.00014515029836329572, 'Error: ', 1.666666666666483e-05)\n",
      "('T: ', 69, 'Loss: ', 0.00014418384665722644, 'Error: ', 1.666666666666483e-05)\n",
      "('T: ', 70, 'Loss: ', 0.00014249083709754536, 'Error: ', 1.666666666666483e-05)\n",
      "('T: ', 71, 'Loss: ', 0.00014164655764231873, 'Error: ', 1.666666666666483e-05)\n",
      "('T: ', 72, 'Loss: ', 0.0001398528648762842, 'Error: ', 1.666666666666483e-05)\n",
      "('T: ', 73, 'Loss: ', 0.00013951382998426392, 'Error: ', 1.666666666666483e-05)\n",
      "('T: ', 74, 'Loss: ', 0.0001380487473718272, 'Error: ', 0.0)\n",
      "('T: ', 75, 'Loss: ', 0.00013807105960473223, 'Error: ', 1.666666666666483e-05)\n",
      "('T: ', 76, 'Loss: ', 0.0001356745509531017, 'Error: ', 0.0)\n",
      "('T: ', 77, 'Loss: ', 0.00013529791265054782, 'Error: ', 1.666666666666483e-05)\n",
      "('T: ', 78, 'Loss: ', 0.00013394614067993102, 'Error: ', 1.666666666666483e-05)\n",
      "('T: ', 79, 'Loss: ', 0.0001336528747358996, 'Error: ', 0.0)\n",
      "('T: ', 80, 'Loss: ', 0.00013292263463151536, 'Error: ', 0.0)\n",
      "('T: ', 81, 'Loss: ', 0.00013110772902294535, 'Error: ', 0.0)\n",
      "('T: ', 82, 'Loss: ', 0.00013045903943908026, 'Error: ', 1.666666666666483e-05)\n",
      "('T: ', 83, 'Loss: ', 0.00012991930686610134, 'Error: ', 0.0)\n",
      "('T: ', 84, 'Loss: ', 0.00012890804183634752, 'Error: ', 0.0)\n",
      "('T: ', 85, 'Loss: ', 0.0001273885501550845, 'Error: ', 0.0)\n",
      "('T: ', 86, 'Loss: ', 0.0001272548801405836, 'Error: ', 0.0)\n",
      "('T: ', 87, 'Loss: ', 0.00012589926168729633, 'Error: ', 0.0)\n",
      "('T: ', 88, 'Loss: ', 0.00012559669605239848, 'Error: ', 0.0)\n",
      "('T: ', 89, 'Loss: ', 0.00012440885117518275, 'Error: ', 0.0)\n",
      "('T: ', 90, 'Loss: ', 0.00012350057309342397, 'Error: ', 0.0)\n",
      "('T: ', 91, 'Loss: ', 0.00012289967410678685, 'Error: ', 0.0)\n",
      "('T: ', 92, 'Loss: ', 0.0001214537488082328, 'Error: ', 0.0)\n",
      "('T: ', 93, 'Loss: ', 0.00012150031245009916, 'Error: ', 0.0)\n",
      "('T: ', 94, 'Loss: ', 0.00012020734617928428, 'Error: ', 0.0)\n",
      "('T: ', 95, 'Loss: ', 0.00011940073067378473, 'Error: ', 0.0)\n",
      "('T: ', 96, 'Loss: ', 0.00011810173589848046, 'Error: ', 0.0)\n",
      "('T: ', 97, 'Loss: ', 0.00011778033486883122, 'Error: ', 0.0)\n",
      "('T: ', 98, 'Loss: ', 0.00011683705226946489, 'Error: ', 0.0)\n",
      "('T: ', 99, 'Loss: ', 0.00011652163154510307, 'Error: ', 0.0)\n",
      "('T: ', 100, 'Loss: ', 0.00011507741198382157, 'Error: ', 0.0)\n",
      "('T: ', 101, 'Loss: ', 0.00011512618832819754, 'Error: ', 0.0)\n",
      "('T: ', 102, 'Loss: ', 0.00011357923994880993, 'Error: ', 0.0)\n",
      "('T: ', 103, 'Loss: ', 0.00011328150805757834, 'Error: ', 0.0)\n",
      "('T: ', 104, 'Loss: ', 0.00011217323619292043, 'Error: ', 0.0)\n",
      "('T: ', 105, 'Loss: ', 0.00011158503425493417, 'Error: ', 0.0)\n",
      "('T: ', 106, 'Loss: ', 0.00011115367530995686, 'Error: ', 0.0)\n",
      "('T: ', 107, 'Loss: ', 0.00010996732606394201, 'Error: ', 0.0)\n",
      "('T: ', 108, 'Loss: ', 0.00010988223715461111, 'Error: ', 0.0)\n",
      "('T: ', 109, 'Loss: ', 0.00010864674987633886, 'Error: ', 0.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('T: ', 110, 'Loss: ', 0.00010821982994713549, 'Error: ', 0.0)\n",
      "('T: ', 111, 'Loss: ', 0.00010763635104129271, 'Error: ', 0.0)\n",
      "('T: ', 112, 'Loss: ', 0.00010685553756335739, 'Error: ', 0.0)\n",
      "('T: ', 113, 'Loss: ', 0.00010646829221895961, 'Error: ', 0.0)\n",
      "('T: ', 114, 'Loss: ', 0.00010527363340205519, 'Error: ', 0.0)\n",
      "('T: ', 115, 'Loss: ', 0.0001046069525864973, 'Error: ', 0.0)\n",
      "('T: ', 116, 'Loss: ', 0.00010448935249847584, 'Error: ', 0.0)\n",
      "('T: ', 117, 'Loss: ', 0.00010326615981103943, 'Error: ', 0.0)\n",
      "('T: ', 118, 'Loss: ', 0.00010304771339384928, 'Error: ', 0.0)\n",
      "('T: ', 119, 'Loss: ', 0.00010181554920515751, 'Error: ', 0.0)\n",
      "('T: ', 120, 'Loss: ', 0.00010186505306922492, 'Error: ', 0.0)\n",
      "('T: ', 121, 'Loss: ', 0.00010151651725879781, 'Error: ', 0.0)\n",
      "('T: ', 122, 'Loss: ', 0.00010035021406999632, 'Error: ', 0.0)\n",
      "('T: ', 123, 'Loss: ', 9.96516019038253e-05, 'Error: ', 0.0)\n",
      "('T: ', 124, 'Loss: ', 9.924603185755066e-05, 'Error: ', 0.0)\n",
      "('T: ', 125, 'Loss: ', 9.846860936777295e-05, 'Error: ', 0.0)\n",
      "('T: ', 126, 'Loss: ', 9.826042122369131e-05, 'Error: ', 0.0)\n",
      "('T: ', 127, 'Loss: ', 9.756037645294998e-05, 'Error: ', 0.0)\n",
      "('T: ', 128, 'Loss: ', 9.72051605940091e-05, 'Error: ', 0.0)\n",
      "('T: ', 129, 'Loss: ', 9.654763150846238e-05, 'Error: ', 0.0)\n",
      "('T: ', 130, 'Loss: ', 9.583272989600368e-05, 'Error: ', 0.0)\n",
      "('T: ', 131, 'Loss: ', 9.549862179491742e-05, 'Error: ', 0.0)\n",
      "('T: ', 132, 'Loss: ', 9.445124051944302e-05, 'Error: ', 0.0)\n",
      "('T: ', 133, 'Loss: ', 9.441792162833822e-05, 'Error: ', 0.0)\n",
      "('T: ', 134, 'Loss: ', 9.39550322178333e-05, 'Error: ', 0.0)\n",
      "('T: ', 135, 'Loss: ', 9.305513498747398e-05, 'Error: ', 0.0)\n",
      "('T: ', 136, 'Loss: ', 9.2924161595085e-05, 'Error: ', 0.0)\n",
      "('T: ', 137, 'Loss: ', 9.167272627815921e-05, 'Error: ', 0.0)\n",
      "('T: ', 138, 'Loss: ', 9.147841045755302e-05, 'Error: ', 0.0)\n",
      "('T: ', 139, 'Loss: ', 9.124881169858543e-05, 'Error: ', 0.0)\n",
      "('T: ', 140, 'Loss: ', 9.001708249109566e-05, 'Error: ', 0.0)\n",
      "('T: ', 141, 'Loss: ', 8.980243724339014e-05, 'Error: ', 0.0)\n",
      "('T: ', 142, 'Loss: ', 8.956549328552427e-05, 'Error: ', 0.0)\n",
      "('T: ', 143, 'Loss: ', 8.889100834172287e-05, 'Error: ', 0.0)\n",
      "('T: ', 144, 'Loss: ', 8.856462916615642e-05, 'Error: ', 0.0)\n",
      "('T: ', 145, 'Loss: ', 8.74943891944881e-05, 'Error: ', 0.0)\n",
      "('T: ', 146, 'Loss: ', 8.792108542059216e-05, 'Error: ', 0.0)\n",
      "('T: ', 147, 'Loss: ', 8.69282047346332e-05, 'Error: ', 0.0)\n",
      "('T: ', 148, 'Loss: ', 8.668005937072718e-05, 'Error: ', 0.0)\n",
      "('T: ', 149, 'Loss: ', 8.612627391182269e-05, 'Error: ', 0.0)\n",
      "('T: ', 150, 'Loss: ', 8.606292960606667e-05, 'Error: ', 0.0)\n",
      "('T: ', 151, 'Loss: ', 8.517941668128983e-05, 'Error: ', 0.0)\n",
      "('T: ', 152, 'Loss: ', 8.47923134604807e-05, 'Error: ', 0.0)\n",
      "('T: ', 153, 'Loss: ', 8.407853689236317e-05, 'Error: ', 0.0)\n",
      "('T: ', 154, 'Loss: ', 8.38445075600448e-05, 'Error: ', 0.0)\n",
      "('T: ', 155, 'Loss: ', 8.326370019337072e-05, 'Error: ', 0.0)\n",
      "('T: ', 156, 'Loss: ', 8.259723522212255e-05, 'Error: ', 0.0)\n",
      "('T: ', 157, 'Loss: ', 8.279797048175105e-05, 'Error: ', 0.0)\n",
      "('T: ', 158, 'Loss: ', 8.225762487108579e-05, 'Error: ', 0.0)\n",
      "('T: ', 159, 'Loss: ', 8.181751843888028e-05, 'Error: ', 0.0)\n",
      "('T: ', 160, 'Loss: ', 8.127281113932634e-05, 'Error: ', 0.0)\n",
      "('T: ', 161, 'Loss: ', 8.099082603848381e-05, 'Error: ', 0.0)\n",
      "('T: ', 162, 'Loss: ', 8.037184230128005e-05, 'Error: ', 0.0)\n",
      "('T: ', 163, 'Loss: ', 8.008258600885847e-05, 'Error: ', 0.0)\n",
      "('T: ', 164, 'Loss: ', 7.947169463673939e-05, 'Error: ', 0.0)\n",
      "('T: ', 165, 'Loss: ', 7.920027860169034e-05, 'Error: ', 0.0)\n",
      "('T: ', 166, 'Loss: ', 7.879999533661997e-05, 'Error: ', 0.0)\n",
      "('T: ', 167, 'Loss: ', 7.840891226667038e-05, 'Error: ', 0.0)\n",
      "('T: ', 168, 'Loss: ', 7.813052293236329e-05, 'Error: ', 0.0)\n",
      "('T: ', 169, 'Loss: ', 7.785559824573516e-05, 'Error: ', 0.0)\n",
      "('T: ', 170, 'Loss: ', 7.739630201666996e-05, 'Error: ', 0.0)\n",
      "('T: ', 171, 'Loss: ', 7.681753047984005e-05, 'Error: ', 0.0)\n",
      "('T: ', 172, 'Loss: ', 7.640728390003945e-05, 'Error: ', 0.0)\n",
      "('T: ', 173, 'Loss: ', 7.593563697598244e-05, 'Error: ', 0.0)\n",
      "('T: ', 174, 'Loss: ', 7.601711191881815e-05, 'Error: ', 0.0)\n",
      "('T: ', 175, 'Loss: ', 7.516363924013416e-05, 'Error: ', 0.0)\n",
      "('T: ', 176, 'Loss: ', 7.506166549778876e-05, 'Error: ', 0.0)\n",
      "('T: ', 177, 'Loss: ', 7.47795829676752e-05, 'Error: ', 0.0)\n",
      "('T: ', 178, 'Loss: ', 7.413564831330459e-05, 'Error: ', 0.0)\n",
      "('T: ', 179, 'Loss: ', 7.383689505981239e-05, 'Error: ', 0.0)\n",
      "('T: ', 180, 'Loss: ', 7.360240140834426e-05, 'Error: ', 0.0)\n",
      "('T: ', 181, 'Loss: ', 7.3224069517684e-05, 'Error: ', 0.0)\n",
      "('T: ', 182, 'Loss: ', 7.292668510920017e-05, 'Error: ', 0.0)\n",
      "('T: ', 183, 'Loss: ', 7.259528406356615e-05, 'Error: ', 0.0)\n",
      "('T: ', 184, 'Loss: ', 7.199452424405413e-05, 'Error: ', 0.0)\n",
      "('T: ', 185, 'Loss: ', 7.177270340740977e-05, 'Error: ', 0.0)\n",
      "('T: ', 186, 'Loss: ', 7.137709339354933e-05, 'Error: ', 0.0)\n",
      "('T: ', 187, 'Loss: ', 7.136472659149392e-05, 'Error: ', 0.0)\n",
      "('T: ', 188, 'Loss: ', 7.07099510275007e-05, 'Error: ', 0.0)\n",
      "('T: ', 189, 'Loss: ', 7.070178416081602e-05, 'Error: ', 0.0)\n",
      "('T: ', 190, 'Loss: ', 7.008337604643877e-05, 'Error: ', 0.0)\n",
      "('T: ', 191, 'Loss: ', 6.979018716790746e-05, 'Error: ', 0.0)\n",
      "('T: ', 192, 'Loss: ', 6.929063504411755e-05, 'Error: ', 0.0)\n",
      "('T: ', 193, 'Loss: ', 6.927515521002665e-05, 'Error: ', 0.0)\n",
      "('T: ', 194, 'Loss: ', 6.89711246333815e-05, 'Error: ', 0.0)\n",
      "('T: ', 195, 'Loss: ', 6.853932695303613e-05, 'Error: ', 0.0)\n",
      "('T: ', 196, 'Loss: ', 6.820607353429036e-05, 'Error: ', 0.0)\n",
      "('T: ', 197, 'Loss: ', 6.794089991134597e-05, 'Error: ', 0.0)\n",
      "('T: ', 198, 'Loss: ', 6.7726620592513e-05, 'Error: ', 0.0)\n",
      "('T: ', 199, 'Loss: ', 6.735472682391669e-05, 'Error: ', 0.0)\n"
     ]
    }
   ],
   "source": [
    "epochtest = backprop_minibatch_shallow(xtrain, dtrain, epochtrain, 200, B=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With T = 10 the test dataset error is 0.02.<br>\n",
    "With T = 100 the training dataset error @ 100 is 0.0037 (minimum of 0.00365). The test dataset error is 8.3e-5.<br>\n",
    "With T = 200, the training set error is 8.3e-05 and the test set error converges to 0 after 82 iterations. This seems unlikely and overfitting may be occuring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
